[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "The Biology to be Explored in LLMs",
    "section": "",
    "text": "This post will be summarizing key points about molecular biology which I have learned from the article “Large Language Models in Molecular Biology” by Serafim Batzoglou.\nIn an attempt to further explore LLMs to prepare for my upcoming project regarding LLMs, the following summary of the biological information in the above article was generated by prompting ChatGPT and edited by me.\n\nCellular and Molecular Biology Components\n\nCentral Dogma\nThe central dogma of molecular biology explains how genetic information flows within living organisms. It states that DNA, which is housed in the nucleus of every cell, is the source of this genetic information. Human DNA consists of approximately 3 billion nucleotides organized into 23 chromosomes, with 22 being autosomes and one being a sex chromosome (X or Y). Each person inherits two nearly identical copies of the human genome, one from each parent. The genetic material from both parents is retained in the nucleus of each of the roughly 30 trillion cells in the human body. The genome contains about 20,000 genes responsible for protein synthesis, with only about 1% of the genome coding for proteins. The remaining portions of the genome include regions that control gene expression, regions within genes that do not code for proteins, regions contributing to DNA structure, and “junk” regions of self-replicating DNA.\nProtein synthesis, a fundamental process in molecular biology, involves three main steps: transcription, splicing, and translation. During transcription, a DNA segment serving as a gene template is copied into messenger RNA (mRNA). The mRNA molecule undergoes splicing, where certain segments, called introns, are removed, and the remaining segments, called exons, are joined together to form mature mRNA. Splicing is crucial in higher organisms because it allows a single gene to produce multiple protein variants by assembling different combinations of exons. The mRNA is then transported to the ribosome, where translation occurs. During translation, the mRNA sequence is decoded into amino acids, which are the building blocks of proteins. These amino acids are linked together to form a protein sequence, which folds into a functional three-dimensional structure. Proteins play essential roles in various biological processes, providing structural components, catalyzing reactions as enzymes, and facilitating communication and transportation within cells.\n\n\nGene Regulation\nGene regulation is a complex process that controls when, where, and in what quantity genes are expressed in cells. It ensures the timely production of the right proteins in appropriate amounts. Gene regulation occurs at different levels, involving chromatin structure, chemical modifications, and the action of transcription factors. Transcription factors are proteins that bind to specific DNA sequences and influence the recruitment of RNA polymerase, the enzyme responsible for mRNA synthesis. They help regulate the expression of target genes to ensure they are appropriately expressed in response to signals.\nPromoters and enhancers are DNA regions that contribute to gene expression control, with promoters located adjacent to gene starts and enhancers situated within introns or between genes, further downstream in the DNA. Chromatin structure, formed by DNA wrapping around histone proteins, determines which DNA regions are accessible for gene expression. Chemical modifications of histones and DNA, such as acetylation, methylation, and DNA methylation, can influence chromatin structure and gene expression. Gene regulation is specific to each type of cell - some cells have certain genes expressed while other cells have different genes expressed. This is what allows cells to have specialized functions.\nThe flow of genetic information is traditionally described as unidirectional: DNA to RNA to protein. However, there are exceptions to this rule. Reverse transcription allows RNA to be converted back into DNA, as seen in retroviruses like HIV. DNA can also be transcribed into different types of RNA, such as transfer RNA (tRNA) and ribosomal RNA (rRNA), adding complexity to genetic information flow.\n\n\nEpigenetic Mechanisms\nEpigenetic mechanisms, including DNA methylation and histone modifications, play a role in gene regulation and can be inherited. DNA methylation is a chemical modification where methyl is added to the DNA molecule, usually at specific cytosine bases. Methylation influences gene expression by affecting the binding of transcription factors and the chromatin structure. Chromatin must be unfolded for gene expression, so by making the chromatin more compact, methylation makes transcription more difficult (affects gene accessibility).\nDNA variation contributes to the diversity and heritability of traits among individuals. DNA variants are introduced primarily through mutations between the genomes of parents and germline genomes passed on to offspring. Deleterious variants tend to be eliminated from the population over time through natural selection. Genetic variations common in humans are typically benign or contribute to diseases that manifest later in life. Some rare mutations can affect the splicing sites (the boundaries where genes are spliced). As a result, they can cause the production of a completely different protein sequence, thus different protein function. This is why they contribute to 10% of rare genetic diseases.\nSo, predicting splice sites and determining gene structure is important to diagnose genetic diseases."
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html",
    "title": "this is a title",
    "section": "",
    "text": "Go to GeneCards for specific gene of interest (use version 19 for now, which is under “Previous Assembly”).\nhttps://www.genecards.org/cgi-bin/carddisp.pl?gene=GSDMB#:~:text=GeneCards%20Summary%20for%20GSDMB%20Gene,of%20this%20gene%20is%20GSDMA. Not the best way to do this, but it works for now.\nFrom GeneCards, look like this is the location: chr17:38,060,848-38,074,888\nLook at enformer.usage Jupyter notebook for the prediction and visualization of these areas.\nKaten Project Goal:\nExamine the activity of histone modifications: - across various tissues and cell lines - across different forms of histone modification - in 17q locus of asthma (most significant locus of asthma)\nLonger-Term Goal: Train genetic predictors of histone modifications - can be combined with GWAS of asthma to identify loci where histone modification may play a role in asthma etiology\n“The 17q21 asthma susceptibility locus is located between 35.0 and 35.5 Mb on chromosome 17 and contains at least 15 genes. To date, however, asthma-associated SNPs have been associated with the expression of only four of these genes: (i) Ikaros zinc finger protein 3 (IKZF3), involved with the regulation of lymphocyte development; (ii) Gasdermin B (GSDMB), implicated in epithelial cell barrier function; (iii) Mediator of RNA polymerase II transcription subunit 24 (MED24), a component of a transcriptional coactivator complex thought to be required for expression of most genes; and (4) ORMDL3, an endoplasmic reticulum (ER) transmembrane protein involved in regulation of sphingolipid metabolism.” Source: https://www.nature.com/articles/pr2013186\nRelevant genes in the 17q21 locus: IKZF3, GSDMB, MED24, ORMDL3. Center at GSDMB.\nHuman Targets (trained ENFORMER):\nhttps://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt\n\nget locus center (17q)\nextract the DNA sequence from the reference genome (part of the enformer function in the hackathon creates the genome sequence) at that locus\ncall enformer (should be just a function) and provide DNA sequence\n\n\nmay want to use parsl to speed up process\n\n\nsave the output in a variable which is 5313 x 896\nplot matrix\nread the human_target.txt or .csv\nanalyze the data from the matrix for indices (columns) at H3K27Ac\n\n\ne.g. like how the hackathon extracts only column 5110\nall histone modification data will be ChIP-seq and will start with H\nH3K27ac is a marker for active enhancers and a great indicator of enhancer activity\nafter enformer outputs the data, it should be just the columns corresponding to the histone modifications\n\n\nplot H3K27Ac in LCL and in other cell types (which are in the targets file)\nprincipal component analysis"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#background-aims",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#background-aims",
    "title": "this is a title",
    "section": "",
    "text": "Go to GeneCards for specific gene of interest (use version 19 for now, which is under “Previous Assembly”).\nhttps://www.genecards.org/cgi-bin/carddisp.pl?gene=GSDMB#:~:text=GeneCards%20Summary%20for%20GSDMB%20Gene,of%20this%20gene%20is%20GSDMA. Not the best way to do this, but it works for now.\nFrom GeneCards, look like this is the location: chr17:38,060,848-38,074,888\nLook at enformer.usage Jupyter notebook for the prediction and visualization of these areas.\nKaten Project Goal:\nExamine the activity of histone modifications: - across various tissues and cell lines - across different forms of histone modification - in 17q locus of asthma (most significant locus of asthma)\nLonger-Term Goal: Train genetic predictors of histone modifications - can be combined with GWAS of asthma to identify loci where histone modification may play a role in asthma etiology\n“The 17q21 asthma susceptibility locus is located between 35.0 and 35.5 Mb on chromosome 17 and contains at least 15 genes. To date, however, asthma-associated SNPs have been associated with the expression of only four of these genes: (i) Ikaros zinc finger protein 3 (IKZF3), involved with the regulation of lymphocyte development; (ii) Gasdermin B (GSDMB), implicated in epithelial cell barrier function; (iii) Mediator of RNA polymerase II transcription subunit 24 (MED24), a component of a transcriptional coactivator complex thought to be required for expression of most genes; and (4) ORMDL3, an endoplasmic reticulum (ER) transmembrane protein involved in regulation of sphingolipid metabolism.” Source: https://www.nature.com/articles/pr2013186\nRelevant genes in the 17q21 locus: IKZF3, GSDMB, MED24, ORMDL3. Center at GSDMB.\nHuman Targets (trained ENFORMER):\nhttps://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt\n\nget locus center (17q)\nextract the DNA sequence from the reference genome (part of the enformer function in the hackathon creates the genome sequence) at that locus\ncall enformer (should be just a function) and provide DNA sequence\n\n\nmay want to use parsl to speed up process\n\n\nsave the output in a variable which is 5313 x 896\nplot matrix\nread the human_target.txt or .csv\nanalyze the data from the matrix for indices (columns) at H3K27Ac\n\n\ne.g. like how the hackathon extracts only column 5110\nall histone modification data will be ChIP-seq and will start with H\nH3K27ac is a marker for active enhancers and a great indicator of enhancer activity\nafter enformer outputs the data, it should be just the columns corresponding to the histone modifications\n\n\nplot H3K27Ac in LCL and in other cell types (which are in the targets file)\nprincipal component analysis"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#setup",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#setup",
    "title": "this is a title",
    "section": "Setup",
    "text": "Setup\nImport necessary packages.\n\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\nfrom sklearn.preprocessing import OneHotEncoder\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2023-07-19 20:36:28.384228: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\nChanged the original code from !pip to %pip because only &pip allows us to import packages into the correct location (including tensorflow, which was the specific package that wasn’t importing here).\n\n#tensorflow import\n%pip install tensorflow \n%pip install tensorflow_hub\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tensorflow in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (2.11.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (4.4.0)\nRequirement already satisfied: packaging in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: h5py&gt;=2.9.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (15.0.6.1)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (2.2.0)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: tensorboard&lt;2.12,&gt;=2.11 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (2.11.2)\nRequirement already satisfied: gast&lt;=0.4.0,&gt;=0.2.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: flatbuffers&gt;=2.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (2.0.7)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (0.30.0)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: protobuf&lt;3.20,&gt;=3.9.2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (3.19.6)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: tensorflow-estimator&lt;2.12,&gt;=2.11.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: keras&lt;2.12,&gt;=2.11.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (2.11.0)\nRequirement already satisfied: six&gt;=1.12.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: numpy&gt;=1.20 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: setuptools in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow) (65.6.3)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.38.4)\nRequirement already satisfied: mpi4py&gt;=3.1.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from h5py&gt;=2.9.0-&gt;tensorflow) (3.1.4)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.16.0)\nRequirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (0.4.6)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.28.1)\nRequirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (0.6.1)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.2.2)\nRequirement already satisfied: markdown&gt;=2.6.8 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (3.4.1)\nRequirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (1.8.1)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from packaging-&gt;tensorflow) (3.0.9)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (0.2.8)\nRequirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (5.3.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (1.3.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (3.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2022.12.7)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (1.26.13)\nRequirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.0.4)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (2.1.1)\nRequirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.12,&gt;=2.11-&gt;tensorflow) (3.2.2)\n\n[notice] A new release of pip is available: 23.0 -&gt; 23.2\n[notice] To update, run: python -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tensorflow_hub in ./.local/polaris/conda/2023-01-10-unstable/lib/python3.10/site-packages (0.13.0)\nRequirement already satisfied: numpy&gt;=1.12.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow_hub) (1.23.5)\nRequirement already satisfied: protobuf&gt;=3.19.6 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from tensorflow_hub) (3.19.6)\n\n[notice] A new release of pip is available: 23.0 -&gt; 23.2\n[notice] To update, run: python -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport tensorflow as tf\n# Make sure the GPU is enabled\nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\n\nkipoiseq is a package that helps us to extract sequences from fasta files given some intervals. We will install the package.\n\n%pip install kipoiseq==0.5.2 --quiet &gt; /dev/null #changed ! in front of pip to % on polaris for package import \n# You can ignore the pyYAML error\n\n\n[notice] A new release of pip is available: 23.0 -&gt; 23.2\n[notice] To update, run: python -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install Biopython #Biopython is a python package that helps us do many bioinfomatic analysis in python\n\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: Biopython in ./.local/polaris/conda/2023-01-10-unstable/lib/python3.10/site-packages (1.81)\nRequirement already satisfied: numpy in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from Biopython) (1.23.5)\n\n[notice] A new release of pip is available: 23.0 -&gt; 23.2\n[notice] To update, run: python -m pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\nWe want to define some paths to save downloaded files for the duration of this notebook.\n\ntransform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\nmodel_path = 'https://tfhub.dev/deepmind/enformer/1' #where DeepMind stored the enformer model\nfasta_file = '/grand/TFXcan/imlab/users/srusti/enformer/data/genome.fa' #contains the reference human genome\n\nWe use B lymphoblastoid cell line predictions here because that is the cell line used to generate GEUVADIS gene expression data. You can copy the https link, paste in another tab in your browser and look through the large txt file for other tracks."
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#extracting-dna-sequence-centered-at-locus",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#extracting-dna-sequence-centered-at-locus",
    "title": "this is a title",
    "section": "Extracting DNA Sequence Centered at Locus",
    "text": "Extracting DNA Sequence Centered at Locus\n\nclass FastaStringExtractor:\n    def __init__(self, fasta_file):\n        import pyfaidx\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    def extract(self, interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        import kipoiseq\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = kipoiseq.Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                            trimmed_interval.start + 1,\n                                            trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n    def close(self):\n        return self.fasta.close()\n\nTo use the above function, first create an instance of the FastaStringExtractor object, providing a fasta file you want to use:\n\nfasta_extractor_object = FastaStringExtractor(fasta_file)\n\nThen, create an interval you want to extract the sequence from using kipoiseq (“.resize” will resize the interval you provide to a length provided by “sequence_length”. The new interval will be centered on the “start”, “end” you provided originally ):\ntarget_interval = kipoiseq.Interval(chrom,start,end).resize(SEQUENCE_LENGTH)\n\nSEQUENCE_LENGTH = 393216\ntarget_interval = kipoiseq.Interval('chr17', 38060848, 38074888).resize(SEQUENCE_LENGTH) #17q locus of asthma\n\n\ntarget_interval\n\nInterval(chrom='chr17', start=37871260, end=38264476, name='', strand='.', ...)\n\n\nFinally, extract the sequence using the extractor object like this:\n\nextracted_sequence = fasta_extractor_object.extract(target_interval)\n\n\n#print(extracted_sequence)\nlen(extracted_sequence)\n\n393216"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#call-enformer-using-extracted-dna-sequence",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#call-enformer-using-extracted-dna-sequence",
    "title": "this is a title",
    "section": "Call Enformer using Extracted DNA Sequence",
    "text": "Call Enformer using Extracted DNA Sequence\nNext, we have some functions that will help us along the way. Classes and methods defined in this code block can be found in the original Enformer usage colab notebook.\nEnformer, EnformerScoreVariantsNormalized, EnformerScoreVariantsPCANormalized\n\n# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\nSEQUENCE_LENGTH = 393216\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n\n\nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n\n# TODO(avsec): Add feature description: Either PCX, or full names.\n\nOne Hot Encoder\nOne-Hot Encoder converts the region of the genome from letters like A, G, T, C to binary (1, 0)\n\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n\ndef variant_centered_sequences(vcf_file, sequence_length, gzipped=False,\n                               chr_prefix=''):\n  seq_extractor = kipoiseq.extractors.VariantSeqExtractor(\n    reference_sequence=FastaStringExtractor(fasta_file))\n\n  for variant in variant_generator(vcf_file, gzipped=gzipped):\n    interval = Interval(chr_prefix + variant.chrom,\n                        variant.pos, variant.pos)\n    interval = interval.resize(sequence_length)\n    center = interval.center() - interval.start\n\n    reference = seq_extractor.extract(interval, [], anchor=center)\n    alternate = seq_extractor.extract(interval, [variant], anchor=center)\n\n    yield {'inputs': {'ref': one_hot_encode(reference),\n                      'alt': one_hot_encode(alternate)},\n           'metadata': {'chrom': chr_prefix + variant.chrom,\n                        'pos': variant.pos,\n                        'id': variant.id,\n                        'ref': variant.ref,\n                        'alt': variant.alt}}\n\nDefining the Enformer model and fasta_extractor in order to run Enformer\n\nmodel = Enformer(model_path)\nfasta_extractor = FastaStringExtractor(fasta_file)\n\nRun One Hot Encoder and run enformer.\n\nsequence_one_hot = one_hot_encode(fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)))\npredictions = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0]\n\n\npredictions.shape #shows that this is the 896 by 5313 matrix that Enformer outputs\n\n(896, 5313)"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#plot-enformer-matrix-896-x-5313",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#plot-enformer-matrix-896-x-5313",
    "title": "this is a title",
    "section": "Plot Enformer Matrix (896 x 5313)",
    "text": "Plot Enformer Matrix (896 x 5313)\nWe will plot this Enformer matrix in R, so it needs to be converted to a CSV file in this notebook first. After saving it as a csv file, we will read the file in RStudio and visualize it using image(matrix), where “matrix” is the name of our Enformer predictions matrix (assigned as ‘predictions’).\n\nimport numpy as np\n\nnp.savetxt('/grand/TFXcan/imlab/users/srusti/enformer/data/q17_enformer.csv', predictions, delimiter=',')\n\nTo save the file from polaris directory to local directory, use the command scp:\nscp sd04@polaris.alcf.anl.gov:/grand/TFXcan/imlab/users/srusti/enformer/data/q17_enformer.csv ~/localfolderpath\n\nprint(predictions.max())\n\n559.01654\n\n\nAttempts at making a plot of the enformer output matrix. Very computationally expensive.\n\n#vmax = predictions.max()\n#sns.heatmap(predictions, vmin=0, vmax=1)"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#plot-tracks",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#plot-tracks",
    "title": "this is a title",
    "section": "Plot Tracks",
    "text": "Plot Tracks\nFunction to plot tracks\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\nDownload targets_human.txt\n\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt' \ndf_targets = pd.read_csv(targets_txt, sep='\\t')\ndf_targets\n\n\n\n\n\n\n\n\nindex\ngenome\nidentifier\nfile\nclip\nscale\nsum_stat\ndescription\n\n\n\n\n0\n0\n0\nENCFF833POA\n/home/drk/tillage/datasets/human/dnase/encode/...\n32\n2\nmean\nDNASE:cerebellum male adult (27 years) and mal...\n\n\n1\n1\n0\nENCFF110QGM\n/home/drk/tillage/datasets/human/dnase/encode/...\n32\n2\nmean\nDNASE:frontal cortex male adult (27 years) and...\n\n\n2\n2\n0\nENCFF880MKD\n/home/drk/tillage/datasets/human/dnase/encode/...\n32\n2\nmean\nDNASE:chorion\n\n\n3\n3\n0\nENCFF463ZLQ\n/home/drk/tillage/datasets/human/dnase/encode/...\n32\n2\nmean\nDNASE:Ishikawa treated with 0.02% dimethyl sul...\n\n\n4\n4\n0\nENCFF890OGQ\n/home/drk/tillage/datasets/human/dnase/encode/...\n32\n2\nmean\nDNASE:GM03348\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5308\n5308\n0\nCNhs14239\n/home/drk/tillage/datasets/human/cage/fantom/C...\n384\n1\nsum\nCAGE:epithelioid sarcoma cell line:HS-ES-2R\n\n\n5309\n5309\n0\nCNhs14240\n/home/drk/tillage/datasets/human/cage/fantom/C...\n384\n1\nsum\nCAGE:squamous cell lung carcinoma cell line:RE...\n\n\n5310\n5310\n0\nCNhs14241\n/home/drk/tillage/datasets/human/cage/fantom/C...\n384\n1\nsum\nCAGE:gastric cancer cell line:GSS\n\n\n5311\n5311\n0\nCNhs14244\n/home/drk/tillage/datasets/human/cage/fantom/C...\n384\n1\nsum\nCAGE:carcinoid cell line:NCI-H727\n\n\n5312\n5312\n0\nCNhs14245\n/home/drk/tillage/datasets/human/cage/fantom/C...\n384\n1\nsum\nCAGE:lung adenocarcinoma, papillary cell line:...\n\n\n\n\n5313 rows × 8 columns\n\n\n\nFunction to extract from dataframe based on substring\n\ndef idx_substr(df, column_name, substring):\n    mask = df[column_name].str.contains(substring)\n    return df[mask].index\n\n\nindices = idx_substr(df_targets, 'description', 'H3K27ac')\narray_h32kac = np.array(indices)\nlist_h32kac = array_h32kac.tolist()\nprint(list_h32kac)\nlen(list_h32kac)\n\n[688, 705, 706, 707, 719, 724, 732, 737, 743, 752, 764, 770, 788, 817, 848, 853, 864, 1107, 1379, 1397, 1408, 1443, 1448, 1450, 1452, 1456, 1470, 1471, 1481, 1487, 1488, 1492, 1494, 1499, 1533, 1534, 1553, 1555, 1579, 1610, 1628, 1648, 1657, 1659, 1671, 1700, 1715, 1741, 1744, 1778, 1789, 1838, 1844, 1871, 1882, 1921, 1938, 1981, 2000, 2008, 2039, 2047, 2057, 2077, 2089, 2090, 2104, 2106, 2111, 2122, 2150, 2155, 2162, 2169, 2170, 2188, 2230, 2234, 2280, 2283, 2285, 2322, 2380, 2385, 2389, 2407, 2410, 2420, 2425, 2433, 2436, 2448, 2454, 2510, 2537, 2572, 2604, 2606, 2640, 2644, 2670, 2683, 2684, 2745, 2761, 2763, 2789, 2795, 2821, 2832, 2833, 2839, 2856, 2862, 2899, 2970, 2976, 2991, 2994, 3019, 3024, 3025, 3060, 3065, 3083, 3108, 3116, 3127, 3138, 3162, 3181, 3183, 3194, 3234, 3242, 3254, 3285, 3296, 3297, 3299, 3300, 3312, 3324, 3330, 3339, 3343, 3360, 3363, 3429, 3432, 3436, 3444, 3457, 3460, 3462, 3475, 3478, 3485, 3501, 3530, 3539, 3542, 3547, 3588, 3592, 3657, 3664, 3670, 3671, 3674, 3686, 3690, 3692, 3702, 3703, 3724, 3725, 3736, 3744, 3747, 3759, 3764, 3773, 3789, 3806, 3824, 3836, 3844, 3851, 3854, 3900, 3925, 3928, 3936, 3946, 3959, 3995, 4004, 4026, 4027, 4036, 4048, 4057, 4089, 4093, 4149, 4152, 4164, 4192, 4194, 4196, 4200, 4245, 4261, 4263, 4266, 4275, 4282, 4312, 4323, 4359, 4378, 4380, 4410, 4427, 4461, 4463, 4477]\n\n\n228\n\n\nThe columns in the ‘predictions’ enformer output matrix is the same as the rows in the human targets dataframe.\n\n#idk why this for loop isn't working (goal is to iterate through the list of indices and plot the tracks for each of those indices)\n#for i in list_h32kac:\n    #tracks = predictions[:, i]\n    #plot_tracks(targets, target_interval)\n\nTrying for loop again but with dictionary comprehension.\n\nhm_targets = df_targets[df_targets[\"description\"].str.contains(\"H3K27ac\")]\nhm_indices = hm_targets.index.to_numpy()\nhm_desc = hm_targets.description\n\n\nlen(hm_desc)\n\n228\n\n\n\nprint(hm_desc)\n\n688                                  CHIP:H3K27ac:GM12878\n705     CHIP:H3K27ac:endothelial cell of umbilical vei...\n706                      CHIP:H3K27ac:keratinocyte female\n707     CHIP:H3K27ac:mammary epithelial cell female ad...\n719     CHIP:H3K27ac:fibroblast of lung female child (...\n                              ...                        \n4410      CHIP:H3K27ac:chorionic villus embryo (16 weeks)\n4427    CHIP:H3K27ac:stomach smooth muscle female adul...\n4461    CHIP:H3K27ac:right lobe of liver female adult ...\n4463    CHIP:H3K27ac:ascending aorta female adult (51 ...\n4477                                   CHIP:H3K27ac:RWPE2\nName: description, Length: 228, dtype: object\n\n\nTaking too long to run, but worked at some point?\n\n#tracks = {}\n#for id, desc in zip(hm_indices, hm_desc[:30]):\n    #tracks[desc] = predictions[:, id]\n\n#plot_tracks(tracks, target_interval)\n\n\ntracks = {}\nfor id, desc in zip(hm_indices, hm_desc[:5]):\n    tracks[desc] = predictions[:, id]\nplot_tracks(tracks, target_interval)\n\n\n\n\nWith the above plots, the y-axis is signal strength and the x-axis is locations along the genome. Keep in mind that the y-axis is different for each plot, so don’t be misguided by that. The peaks indicate a strong signal.\nCreating new matrix of just H32Kac histone modifications.\n\n\ndef spec_matr(matrix, indices):\n    selected_columns = matrix[:, indices]\n    return selected_columns\n\nhist_mat = spec_matr(predictions, hm_indices)\nhist_mat\n\narray([[3.4532719 , 0.2985505 , 0.24109149, ..., 0.19910996, 0.72555166,\n        0.4647423 ],\n       [5.4941144 , 0.3681377 , 0.3043263 , ..., 0.4559032 , 0.8414081 ,\n        0.57398266],\n       [7.0137544 , 0.5032303 , 0.31948483, ..., 0.8457772 , 0.97886765,\n        0.63686115],\n       ...,\n       [0.36366862, 0.24753092, 0.28972805, ..., 2.8498976 , 1.4871123 ,\n        0.6114363 ],\n       [0.52115595, 0.3303019 , 0.4472383 , ..., 3.1200168 , 1.6202507 ,\n        0.6328136 ],\n       [0.55854005, 0.34868008, 0.50969756, ..., 2.7834978 , 1.532995  ,\n        0.7045519 ]], dtype=float32)\n\n\nTranspose Matrix\n\nfinal_hist = hist_mat.transpose()\n\nConducting PCA.\n\nfrom sklearn.decomposition import PCA\n\n# Create an instance of PCA with the desired number of components\nn_components = 2  # Specify the number of components you want to retain\npca = PCA(n_components=n_components)\n\n# Fit the PCA model to the data\npca.fit(final_hist)\n\n# Transform the data to the principal components\ntransformed_data = pca.transform(final_hist)\n\n# Access the principal components and explained variance ratio\nprincipal_components = pca.components_  # The principal components\nexplained_variance_ratio = pca.explained_variance_ratio_  # The explained variance ratio\n\n# Print the shape of the transformed data matrix\nprint(\"Shape of transformed data:\", transformed_data.shape)\n\nShape of transformed data: (228, 2)\n\n\n\nprint(transformed_data)\nprint(principal_components)\n\n[[ 4.29011505e+02  2.22288208e+01]\n [-5.69267349e+01 -1.08432398e+01]\n [-4.69018478e+01 -1.38660784e+01]\n [-4.59444466e+01 -4.11515274e+01]\n [-4.39879303e+01  2.97819366e+01]\n [-5.16800079e+01 -2.73693085e+01]\n [-4.01018677e+01 -5.44725647e+01]\n [-5.65888405e+01 -4.55861664e+00]\n [-3.71873856e+01  1.38566405e-01]\n [-5.18663254e+01  2.18277340e+01]\n [-5.05086784e+01  1.34429960e+01]\n [-4.23644524e+01 -2.61824722e+01]\n [-2.54263420e+01 -1.40245142e+01]\n [-3.33488045e+01 -2.68672199e+01]\n [-4.16338463e+01 -2.23779087e+01]\n [ 2.43114120e+02 -2.46386642e+01]\n [-3.11567650e+01  5.64353085e+00]\n [-4.13973503e+01  3.40232353e+01]\n [-4.34654846e+01 -1.31848164e+01]\n [-4.33576317e+01 -6.40500336e+01]\n [-3.23938026e+01 -2.04059830e+01]\n [-4.50846176e+01 -2.66593494e+01]\n [-3.96889000e+01 -2.76189404e+01]\n [-3.15704689e+01 -2.07621250e+01]\n [-4.81673813e+01 -3.35985641e+01]\n [-3.21420059e+01  5.59731960e+00]\n [ 2.07487473e+02  3.80074501e+01]\n [-5.25247383e+01  1.01850681e+01]\n [-1.68819256e+01  1.38995361e+02]\n [-3.75193024e+01 -4.40471230e+01]\n [-4.32694550e+01 -4.00518074e+01]\n [-3.93787689e+01 -2.75708408e+01]\n [-3.22931404e+01 -9.68780708e+00]\n [-3.71183434e+01 -1.02942162e+01]\n [-4.62695580e+01  6.29523163e+01]\n [-2.60679951e+01  5.49251175e+00]\n [-3.38679352e+01 -2.13475475e+01]\n [-2.66797371e+01  5.17271309e+01]\n [ 1.34053040e+02  2.82612286e+01]\n [-2.63137913e+01 -4.17184448e+01]\n [-5.81946793e+01 -7.33442545e+00]\n [-3.41397667e+01 -4.59948425e+01]\n [-1.92752247e+01 -8.86209393e+00]\n [-4.20858154e+01  6.57025681e+01]\n [-4.65590515e+01 -7.66035604e+00]\n [-3.86708221e+01 -5.30003433e+01]\n [ 3.77290916e+01  6.00367889e+01]\n [-3.63305206e+01 -5.57575760e+01]\n [-3.38381958e+01  9.58611908e+01]\n [-2.04217415e+01 -6.55782928e+01]\n [-3.00451984e+01  1.53085155e+01]\n [-3.50913696e+01  8.83656597e+00]\n [-3.73023872e+01 -4.48953362e+01]\n [ 2.23328876e+01  8.94171600e+01]\n [ 2.73067810e+02  8.77894211e+01]\n [-4.93203239e+01  1.38920984e+01]\n [ 2.66936855e+01 -3.80065613e+01]\n [ 1.44788345e+02  2.84332256e+01]\n [ 8.29699135e+00  5.61712074e+01]\n [-3.65062561e+01 -4.99313812e+01]\n [-3.67821960e+01  5.85734978e+01]\n [ 3.12843689e+02 -2.18938828e+01]\n [-3.68099518e+01 -2.72697277e+01]\n [-4.08568382e+01 -5.78830032e+01]\n [-3.67619514e+01 -4.61754761e+01]\n [-3.80882645e+01 -4.51873504e-02]\n [ 1.03285761e+01  5.37351494e+01]\n [-3.68850555e+01  9.20244122e+00]\n [-4.75946617e+01 -2.46286507e+01]\n [ 5.77541618e+01  1.30170546e+01]\n [ 2.20004532e+02  1.85488834e+01]\n [-3.65023537e+01 -4.42779398e+00]\n [-3.29802208e+01 -2.30454731e+01]\n [-2.68703518e+01  9.58871460e+00]\n [-5.88325405e+00  1.26219185e+02]\n [ 4.40409889e+01  4.79163017e+01]\n [-3.68162270e+01  9.13016510e+00]\n [-5.07439384e+01 -3.06412144e+01]\n [-2.84839840e+01 -2.79091816e+01]\n [-5.00830994e+01 -5.80908623e+01]\n [-3.22056198e+01  9.33245773e+01]\n [-3.49726982e+01 -1.83637562e+01]\n [ 3.94312973e+01  3.78827248e+01]\n [ 7.95510483e+01  9.48300400e+01]\n [ 4.20624817e+02 -1.42585785e+02]\n [ 1.51039017e+02  3.76786346e+01]\n [-4.25562477e+01 -5.97834549e+01]\n [-4.07407074e+01  4.77315102e+01]\n [-2.42843800e+01  3.88061638e+01]\n [-4.37783623e+01 -4.33335037e+01]\n [-3.69794388e+01 -1.31547511e+00]\n [ 1.94745770e+01  3.43488731e+01]\n [-4.54127083e+01  1.03664383e+02]\n [-3.93407555e+01 -6.10990982e+01]\n [-3.70460815e+01 -2.56168900e+01]\n [-4.54350166e+01 -3.08959560e+01]\n [-4.03714600e+01 -3.08495064e+01]\n [-3.92554398e+01 -3.51556320e+01]\n [ 2.53297211e+02 -1.19522297e+00]\n [-3.71794128e+01 -3.21928596e+01]\n [-3.28474121e+01  8.95713234e+00]\n [-3.68372612e+01 -5.10462379e+01]\n [-7.61319733e+00  2.30270600e+00]\n [-4.25794373e+01 -5.74623566e+01]\n [-4.24105911e+01 -6.15009079e+01]\n [-3.60462914e+01 -5.44592209e+01]\n [-2.74201717e+01  1.14738712e+01]\n [-2.90295506e+01  2.01065178e+01]\n [-3.53000870e+01  1.10632835e+02]\n [ 2.22804146e+01  7.03420029e+01]\n [ 3.81587006e+02 -8.41840897e+01]\n [-3.86171341e+01 -7.05622482e+01]\n [ 2.49438915e+01  4.28693085e+01]\n [-1.90403023e+01  4.26871452e+01]\n [-3.79729652e+01 -5.88900833e+01]\n [-4.53931770e+01 -2.09241581e+01]\n [-3.47246704e+01 -1.54767227e+01]\n [-2.59182281e+01  3.76684685e+01]\n [-3.42783661e+01 -4.67825737e+01]\n [-3.76835899e+01 -5.38020897e+01]\n [-4.27226257e+01 -6.09182205e+01]\n [-3.63096123e+01 -1.81886005e+01]\n [-5.07480087e+01 -1.09040413e+01]\n [-3.12425365e+01  2.53056068e+01]\n [-2.55199718e+01  6.13844061e+00]\n [-2.98957500e+01  2.61851673e+01]\n [-2.37498093e+01  1.31912355e+01]\n [ 1.18976578e+02  1.17882862e+01]\n [-5.29053688e+01  2.46011906e+01]\n [-5.59937210e+01  3.36088066e+01]\n [ 8.93202057e+01 -9.59025002e+00]\n [-9.43031311e+00  2.89637909e+01]\n [-5.68391724e+01  2.55285854e+01]\n [ 3.44866638e+02  1.17349083e+02]\n [-4.39301338e+01 -3.89736366e+01]\n [-2.70862427e+01  5.03470182e+00]\n [-5.12988739e+01  1.15099228e+02]\n [-3.41276550e+01  4.52498589e+01]\n [-3.58261261e+01 -5.82838860e+01]\n [ 3.13724762e+02 -5.35617447e+01]\n [-1.93395710e+01 -1.33511820e+01]\n [-3.76825905e+01 -1.95338726e+01]\n [-3.06164055e+01 -1.69042091e+01]\n [-3.73312302e+01 -2.23549194e+01]\n [-3.95572090e+01 -3.32918015e+01]\n [ 1.54116726e+01 -4.22963142e+01]\n [-3.13595695e+01 -2.13842697e+01]\n [-2.62954025e+01 -3.74205780e+01]\n [-9.26836205e+00 -1.79718323e+01]\n [-3.71944695e+01  1.34128590e+01]\n [-2.49401360e+01  6.67443562e+00]\n [-3.42405815e+01  1.08643560e+01]\n [ 5.81709051e+00  2.15994663e+01]\n [-4.26727753e+01 -6.24938965e+01]\n [-3.71272774e+01  2.39180698e+01]\n [-2.41524696e+01  5.49194298e+01]\n [ 2.34967133e+02 -1.14961601e+02]\n [-4.33071709e+01 -5.64499855e-01]\n [-4.08753052e+01 -4.42880592e+01]\n [ 1.15195747e+02  4.04950943e+01]\n [-3.51685715e+01  8.38030777e+01]\n [ 1.11027889e+01  1.15874832e+02]\n [-2.22046967e+01  1.47011906e-01]\n [-3.56695976e+01 -4.88601608e+01]\n [-1.76058502e+01 -9.75904465e+00]\n [-4.30126228e+01 -3.71555099e+01]\n [-3.30353165e+01  4.55916834e+00]\n [-1.58585091e+01  1.74813175e+00]\n [-1.85551815e+01  5.17074432e+01]\n [ 3.34669991e+01  9.39320602e+01]\n [ 1.26497246e+02  5.82719879e+01]\n [ 2.38732254e+02  1.00835098e+02]\n [-3.34099693e+01 -5.78745890e+00]\n [-4.24933929e+01 -5.80559654e+01]\n [-4.28196907e+01 -5.62694473e+01]\n [-3.08744125e+01 -3.22958946e+01]\n [-4.93449554e+01  7.98572998e+01]\n [-2.45137863e+01  1.88400669e+01]\n [ 4.82275269e+02 -1.47587006e+02]\n [-2.96534081e+01  7.96313553e+01]\n [-3.86990471e+01 -5.42709160e+01]\n [-3.46719322e+01 -2.01222973e+01]\n [-3.93394623e+01 -2.59302902e+01]\n [ 2.07254089e+02  1.53769751e+01]\n [-2.97736149e+01 -3.11685238e+01]\n [-4.89034004e+01 -3.03857021e+01]\n [-3.69259377e+01  1.72739735e+01]\n [-4.19947853e+01 -7.78308535e+00]\n [ 3.40358620e+01  5.69028244e+01]\n [-3.71714172e+01 -2.90266418e+01]\n [-4.85661850e+01  4.61616783e+01]\n [-4.06754570e+01 -1.16336899e+01]\n [-3.79456940e+01 -6.11480560e+01]\n [-4.59130630e+01  9.13757553e+01]\n [-4.11263924e+01 -4.71964912e+01]\n [ 7.47286377e+01  1.63571892e+01]\n [-2.90668831e+01 -5.09080410e+00]\n [-5.28644676e+01 -3.58812218e+01]\n [-3.95947113e+01 -6.44072266e+01]\n [ 7.04938889e+01 -4.87674427e+00]\n [ 2.88818569e+01  8.63479538e+01]\n [-4.05269012e+01 -5.78434486e+01]\n [-5.11247902e+01  3.27801132e+01]\n [-3.71980858e+01  9.48750992e+01]\n [ 2.62284576e+02 -6.50317841e+01]\n [-4.25690231e+01 -7.42995148e+01]\n [-4.07853241e+01 -6.97393951e+01]\n [-4.49804039e+01 -5.42197495e+01]\n [-4.50111359e-01  1.47176867e+01]\n [-4.03771973e+01 -2.02594471e+01]\n [ 2.06398285e+02  6.46116867e+01]\n [-1.98057213e+01  7.55206528e+01]\n [-3.96196289e+01 -2.98523693e+01]\n [-3.40873075e+00  6.28067055e+01]\n [-3.52510490e+01 -2.98613930e+01]\n [-5.37423859e+01 -1.32766399e+01]\n [-3.23342705e+01  8.63809490e+00]\n [-3.44745026e+01 -3.47955284e+01]\n [-4.73818130e+01  8.05034332e+01]\n [-4.12652435e+01 -3.32382584e+01]\n [-4.81538353e+01  8.43866043e+01]\n [ 8.89602432e+01 -3.33641472e+01]\n [-3.97471352e+01 -6.37617912e+01]\n [-3.32443542e+01 -3.48025818e+01]\n [-3.85740509e+01  2.28718452e+01]\n [ 1.94738617e+01  1.78730347e+02]\n [-3.60090714e+01  4.54106979e+01]\n [-5.03067207e+01 -4.56402931e+01]]\n[[ 0.0078284   0.01060935  0.01315628 ... -0.00110051 -0.00120933\n  -0.0007786 ]\n [-0.00430644 -0.00433365 -0.00487836 ...  0.00233991  0.00243057\n   0.00122821]]\n\n\nIn the code above, we import the necessary modules and create an instance of the PCA class from scikit-learn. We specify the desired number of components (n_components) as an argument. Next, we fit the PCA model to the data_matrix using the fit() method. Then, we transform the original data to the principal components using the transform() method.\nYou can access the principal components via the components_ attribute and the explained variance ratio via the explained_variance_ratio_ attribute of the PCA object.\nFinally, we print the shape of the transformed data matrix and show the first few rows of the transformed data.\n\n#producing a PCA plot\n\nplt.scatter(transformed_data[:, 0], transformed_data[:, 1])\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.title(\"PCA Plot\")\nplt.show()\n\n\n\n\nPotential further exploration at asthma locus\n\nRemove outliers in data and replot\nLabel some of the outliers in the PCA plot with the cell type\nRegulatory regions (?), look at the variation in the peaks that are not at the specific locus (so at the beginning of each plot).\nPlot the peaks on the left on the PCA plot. Sum of the half of the bins for each plot and plot it on the PCA to see if there’s a pattern."
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#cell-type-specific-histone-modification-analysis",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#cell-type-specific-histone-modification-analysis",
    "title": "this is a title",
    "section": "Cell Type Specific Histone Modification Analysis",
    "text": "Cell Type Specific Histone Modification Analysis\nLung tissue cells:\n\nlung_targets = df_targets[df_targets[\"description\"].str.contains(\"CHIP\") & df_targets[\"description\"].str.contains(\"H3K27ac\") & df_targets[\"description\"].str.contains(\"lung\")]\nlung_indices = lung_targets.index.to_numpy()\nlung_desc = lung_targets.description\n\ntracks = {}\nfor id, desc in zip(lung_indices, lung_desc[:10]):\n    tracks[desc] = predictions[:, id]\nplot_tracks(tracks, target_interval)\n\n\n\n\nT cells (takes too long to run):\n\n#tcell_targets = df_targets[df_targets[\"description\"].str.contains(\"CHIP\") & df_targets[\"description\"].str.contains(\"H3K27ac\") & df_targets[\"description\"].str.contains(\"T cell\")]\n#tcell_indices = tcell_targets.index.to_numpy()\n#tcell_desc = tcell_targets.description\n\n#tracks = {}\n#for id, desc in zip(tcell_indices, tcell_desc[:1]):\n    #tracks[desc] = predictions[:, id]\n#plot_tracks(tracks, target_interval)\n\nHeart tissue (takes too long to run):\n\n#heart_targets = df_targets[df_targets[\"description\"].str.contains(\"CHIP\") & df_targets[\"description\"].str.contains(\"H3K27ac\") & df_targets[\"description\"].str.contains(\"heart\")]\n#heart_indices = heart_targets.index.to_numpy()\n#heart_desc = heart_targets.description\n\n#tracks = {}\n#for id, desc in zip(heart_indices, heart_desc[:5]):\n    #tracks[desc] = predictions[:, id]\n#plot_tracks(tracks, target_interval)"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#exploratory-analysis-of-enformer-predictions-by-haplotype-averaging",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#exploratory-analysis-of-enformer-predictions-by-haplotype-averaging",
    "title": "this is a title",
    "section": "Exploratory Analysis of Enformer Predictions by Haplotype Averaging",
    "text": "Exploratory Analysis of Enformer Predictions by Haplotype Averaging\nEnformer was trained with the reference genome, and with individuals of european ancestry. We are interested to see, instead of using the reference genome, how to use an average genome across a population (population allele frequencies) with the long term goal of recognizing differences between people. This MAY make a difference in the “population-average epigenome” compared to the “regular reference” epigenome. But to do so, we need to figure out if it even makes a difference if you average the genome and run enf vs run each genome and average their outputs.\nQuestion: is enformer linear? To answer: - Run enformer with one averaged haplotype and produce epigenome 1 - Run enformer with each haplotype individually and average the two matrices to produce epigenome 2 - Compare epigenome 1 with epigenome 2"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#predicting-on-average-haplotype",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#predicting-on-average-haplotype",
    "title": "this is a title",
    "section": "Predicting on Average Haplotype",
    "text": "Predicting on Average Haplotype\nChange the run predictions function: Currently it is averaging the predictions from each haplotype individually. We need to change it so that it averages the haplotype before it predicts.\n\ndef run_hapavg_predictions(gene_intervals, tss_dataframe, individuals_list=None):\n  '''\n  Parameters :\n    gene_intervals : the results from calling `collect_intervals`\n    tss_dataframe : a list of the TSSs dataframes i.e. the TSS for the genes in the chromosomes\n    individuals_list : a list of individuals on which we want to make predictions; defaults to None\n\n  Returns :\n    A list of predictions; the first element is the predictions around the TSS for each gene. The second is the prediction across CAGE tracks\n  '''\n\n  gene_output = dict()\n  gene_predictions = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2]) # creates an interval to select the right sequences\n    target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))  # extracts the fasta sequences, and resizes such that it is compatible with the sequence_length\n    window_coords = target_interval.resize(SEQUENCE_LENGTH) # we also need information about the start and end locations after resizing\n    try:\n      cur_gene_vars = pd.read_csv(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n    except:\n      continue\n    individual_results = dict()\n    individual_prediction = dict()\n\n    if isinstance(individuals_list, list) or isinstance(individuals_list, type(np.empty([1, 1]))):\n      use_individuals = individuals_list\n    elif isinstance(individuals_list, type(None)):\n      use_individuals = cur_gene_vars.columns[4:]\n\n    for individual in use_individuals:\n      print('Currently on gene {}, and predicting on individual {}...'.format(gene, individual))\n      # two haplotypes per individual\n      haplo_1 = list(target_fa[:])\n      haplo_2 = list(target_fa[:])\n\n      ref_mismatch_count = 0\n      for i,row in cur_gene_vars.iterrows():\n\n        geno = row[individual].split(\"|\")\n        if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n          continue\n        if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n          continue\n        if geno[0] == \"1\":\n          haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n        if geno[1] == \"1\":\n          haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n    \n\n      # predict on the individual's two haplotypes\n      prediction_1 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_1))[np.newaxis])['human'][0]\n      prediction_2 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_2))[np.newaxis])['human'][0]\n\n      haplo_avg = np.add((one_hot_encode(\"\".join(haplo_1))[np.newaxis]), (one_hot_encode(\"\".join(haplo_2))[np.newaxis])) / 2 #line which averages\n      prediction_avg = model.predict_on_batch(haplo_avg)['human'][0]\n\n      temp_predictions = [prediction_avg[:, 5110]]\n      individual_prediction[individual] = temp_predictions\n\n      # Calculate TSS CAGE expression which correspond to column 5110 of the predictions above\n      temp_list = list()\n\n      pred_prepared_avg = prepare_for_quantify_prediction_per_TSS(predictions=prediction_avg, gene=gene, tss_df=tss_dataframe)\n      tss_predictions_avg = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_prepared_avg['gene_TSS'], cage_predictions=pred_prepared_avg['cage_predictions'])\n\n      temp_list.append(tss_predictions_avg) # results here are a dictionary for each TSS for each haplotype\n\n      individual_results[individual] = temp_list # save for the individual\n\n    gene_output[gene] = individual_results\n    gene_predictions[gene] = individual_prediction\n\n  return([gene_output, gene_predictions])\n\n\n\nFunction to download chromosome bed files and read chromosomes dataframes.\n\nchrom_bed_downloads = pd.read_csv(\"https://uchicago.box.com/shared/static/du77wf31li38tciv8imivwu57svae03p.csv\")\nchrom_bed_downloads.index = chrom_bed_downloads[\"chroms\"]\n\nchrom_bed_downloads.head(22)\n\n\n\n\n\n\n\n\nchroms\nlink\n\n\nchroms\n\n\n\n\n\n\n1\n1\nhttps://uchicago.box.com/shared/static/9q9n4a0...\n\n\n2\n2\nhttps://uchicago.box.com/shared/static/1tk6a3f...\n\n\n3\n3\nhttps://uchicago.box.com/shared/static/77ldwqq...\n\n\n4\n4\nhttps://uchicago.box.com/shared/static/s0g48al...\n\n\n5\n5\nhttps://uchicago.box.com/shared/static/yafgxb1...\n\n\n6\n6\nhttps://uchicago.box.com/shared/static/9vpxc7z...\n\n\n7\n7\nhttps://uchicago.box.com/shared/static/hkru0gi...\n\n\n8\n8\nhttps://uchicago.box.com/shared/static/ruac33s...\n\n\n9\n9\nhttps://uchicago.box.com/shared/static/dfw6gkj...\n\n\n10\n10\nhttps://uchicago.box.com/shared/static/ek50gvt...\n\n\n11\n11\nhttps://uchicago.box.com/shared/static/ryd5ipz...\n\n\n12\n12\nhttps://uchicago.box.com/shared/static/p9bno4m...\n\n\n13\n13\nhttps://uchicago.box.com/shared/static/9i3fmdv...\n\n\n14\n14\nhttps://uchicago.box.com/shared/static/bkbrfph...\n\n\n15\n15\nhttps://uchicago.box.com/shared/static/1mxbg84...\n\n\n16\n16\nhttps://uchicago.box.com/shared/static/9bpr7eq...\n\n\n17\n17\nhttps://uchicago.box.com/shared/static/43equgq...\n\n\n18\n18\nhttps://uchicago.box.com/shared/static/tla8jhz...\n\n\n19\n19\nhttps://uchicago.box.com/shared/static/3wh7bsx...\n\n\n20\n20\nhttps://uchicago.box.com/shared/static/rh8r8s2...\n\n\n21\n21\nhttps://uchicago.box.com/shared/static/o9f6kt3...\n\n\n22\n22\nhttps://uchicago.box.com/shared/static/6wgvugh...\n\n\n\n\n\n\n\n\ndef download_chrom_beds(chromosome, genes, downloads_table=chrom_bed_downloads):\n  '''\n  Downloads bed/variation files for a chromosome and list of genes\n  '''\n#change all the below path info before /data (home/sd04/enformer) according to the pwd\n\n  link = downloads_table.loc[str(chromosome), \"link\"]\n  chr_which = 'chr' + chromosome\n  for gene in genes:\n    if os.path.exists('/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr' + chromosome + '/chr' + chromosome + '_' + gene + '.bed'): # if the file is in the folder, no need to download again\n      continue\n    !curl -L {link} --output /grand/TFXcan/imlab/users/srusti/enformer/data/chr_{chromosome}_bed.tar.gz && cd /grand/TFXcan/imlab/users/srusti/enformer/data/ && tar -zxf /grand/TFXcan/imlab/users/srusti/enformer/data/chr_{chromosome}_bed.tar.gz ./individual_beds/{chr_which}/{chr_which}_{gene}.bed\n\n    # remove the download tar.gz file\n    !rm /grand/TFXcan/imlab/users/srusti/enformer/data/chr_{chromosome}_bed.tar.gz\n\n\ndownload_chrom_beds(chromosome = \"17\", genes = ['GSDMB'])#, 'MED24', 'ORMDL3', 'IKZF3'])\n\n\nchr17_tss = pd.read_table('/grand/TFXcan/imlab/users/srusti/enformer/data/tss_by_chr/chr17_tss_by_gene.txt', sep='\\t')\n\nRun prediction on 10 individuals with averaged haplotypes.\n\nasthma_genes = ['GSDMB'] # our gene of interest\n\nrand_individuals = ['NA11992', 'NA19235', 'NA20770', 'HG00232', 'HG00342', 'NA20502', 'NA19189', 'HG00108', 'HG00380', 'NA12872'] # individuals we are interested in\n\nasthma_chromosomes = ['17'] # the gene is on chromosome 17\n\nchr17_tss_dfs = [chr17_tss] # we use the TSS information\n\nCheck if individuals are present in the chromosome 17 file.\n\ndef check_individuals(path_to_bed_file, list_of_individuals):\n\n  '''\n  Checks if an individual is missing in bed variation files.\n  These individuals should be removed prior to training\n  '''\n\n  myfile = open(path_to_bed_file, 'r')\n  myline = myfile.readline()\n  bed_names = myline.split('\\t')[4:]\n  myfile.close()\n\n  if set(list_of_individuals).issubset(set(bed_names)) == False:\n    missing = list(set(list_of_individuals).difference(bed_names))\n    print('This (or these) individual(s) is/are not present: {}'.format(missing))\n  else:\n    missing = []\n    print('All individuals are present in the bed file.')\n\n  return(missing)\n\n\nmissing = check_individuals(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr17/chr17_IKZF3.bed\", list_of_individuals = rand_individuals)\nmissing = check_individuals(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr17/chr17_GSDMB.bed\", list_of_individuals = rand_individuals)\nmissing = check_individuals(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr17/chr17_MED24.bed\", list_of_individuals = rand_individuals)\nmissing = check_individuals(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr17/chr17_ORMDL3.bed\", list_of_individuals = rand_individuals)\n\n\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\nAll individuals are present in the bed file.\n\n\nCollect individuals we want to predict for.\n\ndef collect_intervals(chromosomes = [\"22\"], gene_list=None):\n\n  '''\n    Parameters :\n      chromosomes : a list of chromosome numbers; each element should be a string format\n      gene_list : a list of genes; the genes should be located on those chromosomes\n\n    Returns :\n      A dictionary of genes (from gene_list) and their intervals within their respective chromosomes\n  '''\n\n  gene_intervals = {} # Collect intervals for our genes of interest\n\n  for chrom in chromosomes:\n    with open(\"/grand/TFXcan/imlab/users/srusti/enformer/data/gene_chroms/gene_\"+ chrom + \".txt\", \"r\") as chrom_genes:\n      for line in chrom_genes:\n        split_line = line.strip().split(\"\\t\")\n        gene_intervals[split_line[2]] = [\n                                          split_line[0],\n                                          int(split_line[3]),\n                                          int(split_line[4])\n                                        ]\n\n  if isinstance(gene_list, list): # if the user has supplied a list of genes they are interested in\n    use_genes = dict((k, gene_intervals[k]) for k in gene_list if k in gene_intervals)\n    return(use_genes)\n  elif isinstance(gene_list, type(None)):\n    return(gene_intervals)\n\n\nasthma_intervals = collect_intervals(chromosomes=asthma_chromosomes, gene_list=asthma_genes) # here, we collect the intervals for that gene\nasthma_intervals\n\n{'GSDMB': ['17', 38060848, 38077313]}\n\n\nMore specific predictions functions from original hackathon code.\n\ndef prepare_for_quantify_prediction_per_TSS(predictions, gene, tss_df):\n\n  '''\n\n  Parameters:\n          predicitions (A numpy array): All predictions from the track\n          gene (a gene name, character): a gene\n          tss_df: a list of dataframe of genes and their transcription start sites\n  Returns:\n          A dictionary of cage experiment predictions and a list of transcription start sites\n\n  '''\n\n  output = dict()\n  for tdf in tss_df:\n    if gene not in tdf.genes.values:\n      continue\n    gene_tss_list = tdf[tdf.genes == gene].txStart_Sites.apply(str).values\n    gene_tss_list = [t.split(', ') for t in gene_tss_list]\n    gene_tss_list = [int(item) for nestedlist in gene_tss_list for item in nestedlist]\n    gene_tss_list = list(set(gene_tss_list))\n  output['cage_predictions'] = predictions[:, 5110] # a numpy array\n  output['gene_TSS'] = gene_tss_list # a list\n\n\n  return(output) # a dictionary\n\ndef quantify_prediction_per_TSS(low_range, TSS, cage_predictions):\n\n  '''\n  Parameters:\n          low_range (int): The lower interval\n          TSS (list of integers): A list of TSS for a gene\n          cage_predictions: A 1D numpy array or a vector of predictions from enformer corresponding to track 5110 or CAGE predictions\n  Returns:\n          A dictionary of gene expression predictions for each TSS for a gene\n    '''\n  tss_predictions = dict()\n  for tss in TSS:\n    bin_start = low_range + ((768 + 320) * 128)\n    count = -1\n    while bin_start &lt; tss:\n      bin_start = bin_start + 128\n      count += 1\n    if count &gt;= len(cage_predictions)-1:\n      continue\n    cage_preds = cage_predictions[count - 1] + cage_predictions[count] + cage_predictions[count + 1]\n    tss_predictions[tss] = cage_preds\n\n  return(tss_predictions)\n\n\npred_hapl_avg = run_hapavg_predictions(gene_intervals=asthma_intervals, tss_dataframe=chr17_tss_dfs, individuals_list=rand_individuals)\n\nCurrently on gene GSDMB, and predicting on individual NA11992...\nCurrently on gene GSDMB, and predicting on individual NA19235...\nCurrently on gene GSDMB, and predicting on individual NA20770...\nCurrently on gene GSDMB, and predicting on individual HG00232...\nCurrently on gene GSDMB, and predicting on individual HG00342...\nCurrently on gene GSDMB, and predicting on individual NA20502...\nCurrently on gene GSDMB, and predicting on individual NA19189...\nCurrently on gene GSDMB, and predicting on individual HG00108...\nCurrently on gene GSDMB, and predicting on individual HG00380...\nCurrently on gene GSDMB, and predicting on individual NA12872..."
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#regular-enformer-predictions-individual-haplotype-predictions",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#regular-enformer-predictions-individual-haplotype-predictions",
    "title": "this is a title",
    "section": "Regular Enformer Predictions (individual haplotype predictions)",
    "text": "Regular Enformer Predictions (individual haplotype predictions)\nThe regular run_predictions functions outputs the predictions for each haplotype and puts each in a list (without averaging it), which is why it produces double the amount of values as your avg_run_predictions function. Use the regular run_predictions and then average them.\n\ndef run_predictions(gene_intervals, tss_dataframe, individuals_list=None):\n  '''\n  Parameters :\n    gene_intervals : the results from calling `collect_intervals`\n    tss_dataframe : a list of the TSSs dataframes i.e. the TSS for the genes in the chromosomes\n    individuals_list : a list of individuals on which we want to make predictions; defaults to None\n\n  Returns :\n    A list of predictions; the first element is the predictions around the TSS for each gene. The second is the prediction across CAGE tracks\n  '''\n\n  gene_output = dict()\n  gene_predictions = dict()\n\n  for gene in gene_intervals.keys():\n    gene_interval = gene_intervals[gene]\n    target_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2]) # creates an interval to select the right sequences\n    target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))  # extracts the fasta sequences, and resizes such that it is compatible with the sequence_length\n    window_coords = target_interval.resize(SEQUENCE_LENGTH) # we also need information about the start and end locations after resizing\n    try:\n      cur_gene_vars = pd.read_csv(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n    except:\n      continue\n    individual_results = dict()\n    individual_prediction = dict()\n\n    if isinstance(individuals_list, list) or isinstance(individuals_list, type(np.empty([1, 1]))):\n      use_individuals = individuals_list\n    elif isinstance(individuals_list, type(None)):\n      use_individuals = cur_gene_vars.columns[4:]\n\n    for individual in use_individuals:\n      print('Currently on gene {}, and predicting on individual {}...'.format(gene, individual))\n      # two haplotypes per individual\n      haplo_1 = list(target_fa[:])\n      haplo_2 = list(target_fa[:])\n\n      ref_mismatch_count = 0\n      for i,row in cur_gene_vars.iterrows():\n\n        geno = row[individual].split(\"|\")\n        if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n          continue\n        if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n          continue\n        if geno[0] == \"1\":\n          haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n        if geno[1] == \"1\":\n          haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n\n      # predict on the individual's two haplotypes\n      prediction_1 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_1))[np.newaxis])['human'][0]\n      prediction_2 = model.predict_on_batch(one_hot_encode(\"\".join(haplo_2))[np.newaxis])['human'][0]\n      prediction_sep_avg = (prediction_1 + prediction_2)/2\n\n      temp_predictions = prediction_sep_avg[:, 5110] # CAGE predictions we are interested in\n      individual_prediction[individual] = temp_predictions\n\n      # Calculate TSS CAGE expression which correspond to column 5110 of the predictions above\n      temp_list = list()\n\n      pred_sep_avg_prepared = prepare_for_quantify_prediction_per_TSS(predictions=prediction_sep_avg, gene=gene, tss_df=tss_dataframe)\n      tss_pred_sep_avg = quantify_prediction_per_TSS(low_range = window_coords.start, TSS=pred_sep_avg_prepared['gene_TSS'], cage_predictions=pred_sep_avg_prepared['cage_predictions'])\n\n      temp_list.append(tss_pred_sep_avg) # results here are a dictionary for each TSS for the average haplotype\n\n      individual_results[individual] = temp_list # save for the individual\n\n    gene_output[gene] = individual_results\n    gene_predictions[gene] = individual_prediction\n\n  return([gene_output, gene_predictions])\n\n\npred_hapl_sep = run_predictions(gene_intervals=asthma_intervals, tss_dataframe=chr17_tss_dfs, individuals_list=rand_individuals)\n\nCurrently on gene GSDMB, and predicting on individual NA11992...\nCurrently on gene GSDMB, and predicting on individual NA19235...\nCurrently on gene GSDMB, and predicting on individual NA20770...\nCurrently on gene GSDMB, and predicting on individual HG00232...\nCurrently on gene GSDMB, and predicting on individual HG00342...\nCurrently on gene GSDMB, and predicting on individual NA20502...\nCurrently on gene GSDMB, and predicting on individual NA19189...\nCurrently on gene GSDMB, and predicting on individual HG00108...\nCurrently on gene GSDMB, and predicting on individual HG00380...\nCurrently on gene GSDMB, and predicting on individual NA12872...\n\n\n\n\nfrom typing import Iterable\n\ndef get_all_values(d):\n    if isinstance(d, dict):\n        for v in d.values():\n            yield from get_all_values(v)\n    elif isinstance(d, Iterable) and not isinstance(d, str): # or list, set, ... only\n        for v in d:\n            yield from get_all_values(v)\n    else:\n        yield d \n\nvalues_avg_hapl = list(get_all_values(pred_hapl_avg))\nvalues_sep_hapl = list(get_all_values(pred_hapl_sep))\n\n\nprint(len(values_avg_hapl))\nprint(len(values_sep_hapl))\n\n9090\n9090\n\n\n\nplt.scatter(values_avg_hapl, values_sep_hapl)\nplt.xlabel(\"Averaged Haplotype\")\nplt.ylabel(\"Separate Haplotype\")\nplt.title(\"Enformer Predictions: Two Haplotypes vs Averaged Haplotype\")\n\nText(0.5, 1.0, 'Enformer Predictions: Two Haplotypes vs Averaged Haplotype')\n\n\n\n\n\n\n#calculate correlation coefficient for above plot\n\nr = np.corrcoef(values_avg_hapl, values_sep_hapl)[0][1]\ncorr = np.around(r, decimals=5)\nprint(corr)\n\n0.99999\n\n\nThe correlation is extremely close to 1, indicating that the predictions for an averaged haplotype and two separate haplotypes are pretty much the same."
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#prepare-input-data",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#prepare-input-data",
    "title": "this is a title",
    "section": "Prepare Input Data",
    "text": "Prepare Input Data\n\ngeuvadis_gene_expression = pd.read_table('https://uchicago.box.com/shared/static/5vwc7pjw9qmtv7298c4rc7bcuicoyemt.gz', sep='\\t',\n                                         dtype={'gene_id': str, 'gene_name':str, 'TargetID':str, 'Chr':str})\ngeuvadis_gene_expression.head(5)\n\n\n\n\n\n\n\n\ngene_id\ngene_name\nTargetID\nChr\nCoord\nHG00096\nHG00097\nHG00099\nHG00100\nHG00101\n...\nNA20810\nNA20811\nNA20812\nNA20813\nNA20814\nNA20815\nNA20816\nNA20819\nNA20826\nNA20828\n\n\n\n\n0\nENSG00000223972.4\nDDX11L1\nENSG00000223972.4\n1\n11869\n0.320818\n0.344202\n0.354225\n0.478064\n-0.102815\n...\n1.008605\n0.384489\n0.581284\n0.513981\n0.667449\n0.350890\n0.186103\n-0.037976\n0.405439\n0.199143\n\n\n1\nENSG00000227232.3\nWASH7P\nENSG00000227232.3\n1\n29806\n33.714457\n20.185174\n18.095407\n24.100871\n29.018719\n...\n30.980194\n34.086207\n39.678442\n29.643513\n27.120420\n29.121624\n31.117198\n32.047074\n22.798959\n23.563874\n\n\n2\nENSG00000243485.1\nMIR1302-11\nENSG00000243485.1\n1\n29554\n0.240408\n0.157456\n0.218806\n0.320878\n0.067833\n...\n0.065940\n0.228784\n0.140642\n0.283905\n0.273821\n0.286311\n0.324060\n0.049574\n0.255288\n0.157440\n\n\n3\nENSG00000238009.2\nRP11-34P13.7\nENSG00000238009.2\n1\n133566\n0.328272\n0.327932\n0.090064\n0.420443\n0.220269\n...\n0.274071\n0.384179\n0.533693\n0.307221\n0.307367\n0.400278\n0.612321\n0.666633\n0.281138\n1.346129\n\n\n4\nENSG00000239945.1\nRP11-34P13.8\nENSG00000239945.1\n1\n91105\n0.332171\n-0.032164\n0.017323\n0.424677\n0.214025\n...\n0.347323\n0.346744\n0.073580\n0.400396\n0.470517\n0.069749\n0.299353\n0.090019\n0.282554\n-0.157170\n\n\n\n\n5 rows × 467 columns\n\n\n\n\ngene_intervals = collect_intervals(chromosomes=['17'], gene_list=['GSDMB'])\nprint(gene_intervals)\n\n{'GSDMB': ['17', 38060848, 38077313]}\n\n\n\nmodel = Enformer(model_path) # here we load the model architecture.\n\nfasta_extractor = FastaStringExtractor(fasta_file) # we define a class called fasta_extractor to help us extra raw sequence data"
  },
  {
    "objectID": "posts/2023-07-19-katenproj/enf_proj_partone.html#predicting-separated-haplotype-and-averages-haplotype-across-tracks",
    "href": "posts/2023-07-19-katenproj/enf_proj_partone.html#predicting-separated-haplotype-and-averages-haplotype-across-tracks",
    "title": "this is a title",
    "section": "Predicting Separated Haplotype and Averages Haplotype Across Tracks",
    "text": "Predicting Separated Haplotype and Averages Haplotype Across Tracks\nPick one individual at random.\n\nrand_individual = np.random.choice(a=geuvadis_gene_expression.columns[6:-1], replace=False) # individuals we are interested in\nrand_individual\n\n'HG00306'\n\n\n\ngene = 'GSDMB'\ngene_interval = gene_intervals[gene]\ntarget_interval = kipoiseq.Interval(\"chr\" + gene_interval[0],\n                                        gene_interval[1],\n                                        gene_interval[2])\ntarget_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\nwindow_coords = target_interval.resize(SEQUENCE_LENGTH)\ncur_gene_vars = pd.read_csv(\"/grand/TFXcan/imlab/users/srusti/enformer/data/individual_beds/chr\" + gene_interval[0] + \"/chr\" + gene_interval[0] + \"_\"+ gene + \".bed\", sep=\"\\t\", header=0) # read in the appropriate bed file for the gene\n\n\ndef geno_to_seq(gene, individual):\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  ref_mismatch_count = 0\n  for i,row in cur_gene_vars.iterrows():\n\n    geno = row[individual].split(\"|\")\n    if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n      continue\n    if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n      continue\n    if geno[0] == \"1\":\n      haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n    if geno[1] == \"1\":\n      haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n  return haplo_1, haplo_2\n\n\nhaplo_1, haplo_2 = geno_to_seq('GSDMB', rand_individual)\n\nhaplo_1_enc = one_hot_encode(\"\".join(haplo_1))[np.newaxis]\nhaplo_2_enc = one_hot_encode(\"\".join(haplo_2))[np.newaxis]\naverage_enc = np.add(haplo_1_enc, haplo_2_enc) / 2\n\n\nprediction_1 = model.predict_on_batch(haplo_1_enc)['human'][0]\nprediction_2 = model.predict_on_batch(haplo_2_enc)['human'][0]\n\npre_average = model.predict_on_batch(average_enc)['human'][0]\npost_average = (prediction_1 + prediction_2) / 2\n\n\nres = []\nfor i in range(5313):\n    pre_track = pre_average[:, i]\n    post_track = post_average[:, i]\n    corr_all = np.corrcoef(pre_track, post_track)[0][1]\n    res.append(corr_all)\n\n\nprint(min(res), max(res))\n\n0.9998775696710487 0.9999999981804293\n\n\nThe results from both methods are nearly identical across all tracks."
  },
  {
    "objectID": "posts/population-stratification/index.html",
    "href": "posts/population-stratification/index.html",
    "title": "GWAS Code",
    "section": "",
    "text": "GWAS is a genome-wide association study. It observes the differences in genes by a scanning the genomes of a large number of individuals to identify genetic markers, such as single nucleotide polymorphisms (SNPs), that are associated with a specific trait or condition.\nThe process of conducting a GWAS typically involves comparing the genomes of individuals with a particular trait or disease to those without the trait or disease (known as the reference genome – a genome widely agreed upon scientists in the field to be the “control” genome in which other genomes are compared). By identifying genetic markers that are more frequently present in the group with the trait or disease, researchers can infer potential associations between specific genetic variants and the trait of interest.These associations between genes and traits includes traits associated with a disease or condition.\nHowever, it is important to note that GWAS findings often identify statistical associations between genetic markers and traits, rather than direct causal relationships. The inability to identify a direct causal relationship between genetic markers and traits in a GWAS is because most genetic variants do not change the coding of proteins (which carry out the cell’s functions, which can result in a specific trait).\nFurther research and functional studies are typically required to validate and understand the biological significance of these associations. However, GWAS is a good first-step in identifying genes highly associated with a trait in order to pursue next steps of causal identification."
  },
  {
    "objectID": "posts/population-stratification/index.html#genome-wide-association-study-gwas",
    "href": "posts/population-stratification/index.html#genome-wide-association-study-gwas",
    "title": "GWAS Code",
    "section": "",
    "text": "GWAS is a genome-wide association study. It observes the differences in genes by a scanning the genomes of a large number of individuals to identify genetic markers, such as single nucleotide polymorphisms (SNPs), that are associated with a specific trait or condition.\nThe process of conducting a GWAS typically involves comparing the genomes of individuals with a particular trait or disease to those without the trait or disease (known as the reference genome – a genome widely agreed upon scientists in the field to be the “control” genome in which other genomes are compared). By identifying genetic markers that are more frequently present in the group with the trait or disease, researchers can infer potential associations between specific genetic variants and the trait of interest.These associations between genes and traits includes traits associated with a disease or condition.\nHowever, it is important to note that GWAS findings often identify statistical associations between genetic markers and traits, rather than direct causal relationships. The inability to identify a direct causal relationship between genetic markers and traits in a GWAS is because most genetic variants do not change the coding of proteins (which carry out the cell’s functions, which can result in a specific trait).\nFurther research and functional studies are typically required to validate and understand the biological significance of these associations. However, GWAS is a good first-step in identifying genes highly associated with a trait in order to pursue next steps of causal identification."
  },
  {
    "objectID": "posts/population-stratification/index.html#exploring-gwas-using-a-smaller-genomic-dataset.",
    "href": "posts/population-stratification/index.html#exploring-gwas-using-a-smaller-genomic-dataset.",
    "title": "GWAS Code",
    "section": "Exploring GWAS using a smaller genomic dataset.",
    "text": "Exploring GWAS using a smaller genomic dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(devtools)\n\nLoading required package: usethis\n\ndevtools::source_gist(\"38431b74c6c0bf90c12f\")\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"7534a04652d34154de13d2fa2ac042adb0d1f564\"\n\n\n\n#downloading data\n\nif(!file.exists(glue::glue(\"~/Downloads/analysis_population_structure.tgz\")))\n{\n  system(glue::glue(\"wget -O ~/Downloads/analysis_population_structure.tgz https://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\"))\n  ## tar -xf file_name.tar.gz --directory /target/directory\n  system(glue::glue(\"tar xvf ~/Downloads/analysis_population_structure.tgz --directory ~/Downloads/\")) \n}\n\nWarning in system(glue::glue(\"wget -O\n~/Downloads/analysis_population_structure.tgz\nhttps://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\")):\nerror in running command\n\nwork.dir =\"~/Downloads/analysis_population_structure/\"\n\n\n#testing Hardy-Weinberg Equilibrium with population structure\npopinfo = read_tsv(paste0(work.dir,\"relationships_w_pops_051208.txt\"))\n\nRows: 1301 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): FID, IID, dad, mom, population\ndbl (2): sex, pheno\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopinfo %&gt;% count(population)\n\n# A tibble: 11 × 2\n   population     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 ASW           90\n 2 CEU          180\n 3 CHB           90\n 4 CHD          100\n 5 GIH          100\n 6 JPT           91\n 7 LWK          100\n 8 MEX           90\n 9 MKK          180\n10 TSI          100\n11 YRI          180\n\nsamdata = read_tsv(paste0(work.dir,\"phase3_corrected.psam\"),guess_max = 2500)  \n\nRows: 2504 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): #IID, PAT, MAT, SuperPop, Population\ndbl (1): SEX\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsuperpop = samdata %&gt;% select(SuperPop,Population) %&gt;% unique()\nsuperpop = rbind(superpop, data.frame(SuperPop=c(\"EAS\",\"HIS\",\"AFR\"),Population=c(\"CHD\",\"MEX\",\"MKK\")))\n\n\n## what happens if we calculate HWE with this mixed population?\nif(!file.exists(glue::glue(\"{work.dir}output/allhwe.hwe\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --out {work.dir}output/allhwe\"))\nallhwe = read.table(glue::glue(\"{work.dir}output/allhwe.hwe\"),header=TRUE,as.is=TRUE)\nhist(allhwe$P)\n\n\n\n\n\nqqunif(allhwe$P,main='HWE HapMap3 All Pop')\n\nWarning in qqunif(allhwe$P, main = \"HWE HapMap3 All Pop\"): thresholding p to\n1e-30\n\n\n\n\n\n\npop = \"CHB\"\npop = \"CEU\"\npop = \"YRI\"\nfor(pop in c(\"CHB\",\"CEU\",\"YRI\"))\n{\n  ## what if we calculate with single population?\n  popinfo %&gt;% filter(population==pop) %&gt;%\n    write_tsv(path=glue::glue(\"{work.dir}{pop}.fam\") )\n  if(!file.exists(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\")))\n  system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --keep {work.dir}{pop}.fam --out {work.dir}output/hwe-{pop}\"))\n  pophwe = read.table(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\"),header=TRUE,as.is=TRUE)\n  hist(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n  qqunif(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n}\n\nWarning: The `path` argument of `write_tsv()` is deprecated as of readr 1.4.0.\nℹ Please use the `file` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#effect of population stratification on GWAS\n## read igrowth\nigrowth = read_tsv(\"https://raw.githubusercontent.com/hakyimlab/igrowth/master/rawgrowth.txt\", show_col_types = FALSE)\n\n\n## add FID to igrowth file\nigrowth = popinfo %&gt;% select(-pheno) %&gt;% inner_join(igrowth %&gt;% select(IID,growth), by=c(\"IID\"=\"IID\"))\nwrite_tsv(igrowth,path=glue::glue(\"{work.dir}igrowth.pheno\"))\nigrowth %&gt;% ggplot(aes(population,growth)) + geom_violin(aes(fill=population)) + geom_boxplot(width=0.2,col='black',fill='gray',alpha=.8) + theme_bw(base_size = 15)\n\nWarning: Removed 130 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 130 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\nsummary( lm(growth~population,data=igrowth) )\n\n\nCall:\nlm(formula = growth ~ population, data = igrowth)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-58821 -18093  -2242  15896  98760 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    73080.8      938.2  77.894  &lt; 2e-16 ***\npopulationCEU  -2190.1     1175.4  -1.863   0.0625 .  \npopulationCHB   9053.1     2043.9   4.429 9.73e-06 ***\npopulationJPT   3476.8     2034.8   1.709   0.0876 .  \npopulationYRI  -7985.2     1137.2  -7.022 2.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24160 on 3591 degrees of freedom\n  (130 observations deleted due to missingness)\nMultiple R-squared:  0.0345,    Adjusted R-squared:  0.03342 \nF-statistic: 32.08 on 4 and 3591 DF,  p-value: &lt; 2.2e-16\n\n\n\n## run plink linear regression only if it hasn't been run already\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --maf 0.05 --out {work.dir}output/igrowth\"))\nigrowth.assoc = read.table(glue::glue(\"{work.dir}output/igrowth.assoc.linear\"),header=T,as.is=T)\nhist(igrowth.assoc$P)\n\n\n\n\n\nqqunif(igrowth.assoc$P)\n\n\n\n\n\n## install.packages(\"qqman\")\nlibrary(qqman)\n\n\n\n\nFor example usage please run: vignette('qqman')\n\n\n\n\n\nCitation appreciated but not required:\n\n\nTurner, (2018). qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots. Journal of Open Source Software, 3(25), 731, https://doi.org/10.21105/joss.00731.\n\n\n\n\nmanhattan(igrowth.assoc, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\" )\n\n\n\n\n\n## generate PCs using plink\nif(!file.exists(glue::glue(\"{work.dir}output/pca.eigenvec\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --pca --out {work.dir}output/pca\"))\n## read plink calculated PCs\npcplink = read.table(glue::glue(\"{work.dir}output/pca.eigenvec\"),header=F, as.is=T)\nnames(pcplink) = c(\"FID\",\"IID\",paste0(\"PC\", c(1:(ncol(pcplink)-2))) )\npcplink = popinfo %&gt;% left_join(superpop,by=c(\"population\"=\"Population\")) %&gt;% inner_join(pcplink, by=c(\"FID\"=\"FID\", \"IID\"=\"IID\"))\n## plot PC1 vs PC2\npcplink %&gt;% ggplot(aes(PC1,PC2,col=population,shape=SuperPop)) + geom_point(size=3,alpha=.7) + theme_bw(base_size = 15)\n\n\n\n\n\n#running igrowth GWAS using PCs\n\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --covar {work.dir}output/pca.eigenvec --covar-number 1-4 --hide-covar --maf 0.05 --out {work.dir}output/igrowth-adjPC\"))\nigrowth.adjusted.assoc = read.table(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\"),header=T,as.is=T)\n##indadd = igrowth.adjusted.assoc$TEST==\"ADD\"\ntitulo = \"igrowh association adjusted for PCs\"\nhist(igrowth.adjusted.assoc$P,main=titulo)\n\n\n\n\n\nqqunif(igrowth.adjusted.assoc$P,main=titulo)"
  },
  {
    "objectID": "posts/practice_analysis/diabetes_analysis.html",
    "href": "posts/practice_analysis/diabetes_analysis.html",
    "title": "Linear Regression & Data Analysis Basics",
    "section": "",
    "text": "Resources for basic data analysis using python, and conducting a linear regression. Will be using a diabetes dataset.\nimport numpy as np  # library used for working with arrays\nimport pandas as pd # library used for data manipulation and analysis\n\nimport seaborn as sns # library for visualization\nimport matplotlib.pyplot as plt # library for visualization\n%matplotlib inline\n\n\n# to suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n#read csv dataset\n\npima = pd.read_csv(\"diabetes.csv\") # load and reads the csv file\npima\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n763\n10\n101\n76\n48\n180\n32.9\n0.171\n63\n0\n\n\n764\n2\n122\n70\n27\n0\n36.8\n0.340\n27\n0\n\n\n765\n5\n121\n72\n23\n112\n26.2\n0.245\n30\n0\n\n\n766\n1\n126\n60\n0\n0\n30.1\n0.349\n47\n1\n\n\n767\n1\n93\n70\n31\n0\n30.4\n0.315\n23\n0\n\n\n\n\n768 rows × 9 columns\n# finds the number of columns in the dataset\ntotal_cols=len(pima.axes[1])\nprint(\"Number of Columns: \"+str(total_cols))\n\nNumber of Columns: 9\n#first 10 rows\npima.head(10)\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n5\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\n\n\n6\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\n\n\n7\n10\n115\n0\n0\n0\n35.3\n0.134\n29\n0\n\n\n8\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\n\n\n9\n8\n125\n96\n0\n0\n0.0\n0.232\n54\n1\nOutcomes here represents whether someone has diabetes (1) or not (0).\n#number of rows in dataset\n\n# finds the number of rows in the dataset\ntotal_rows=len(pima.axes[0])\nprint(\"Number of Rows: \"+str(total_rows))\n\nNumber of Rows: 768\nprint('The dimension of the DataFrame is: ', pima.ndim)\n\nThe dimension of the DataFrame is:  2\n#size of the dataset (how many elements)\n\npima.size\n\n6912\n#The info() function is used to print a concise summary of a DataFrame.\n#This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\n\npima.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n#functions that return a boolean value indicating whether the passed in argument value is in fact missing data.\n# this is an example of chaining methods\n\npima.isnull().values.any()\n\nFalse\n#it can also output if there is any missing values each of the columns\n\npima.isnull().any()\n\nPregnancies                 False\nGlucose                     False\nBloodPressure               False\nSkinThickness               False\nInsulin                     False\nBMI                         False\nDiabetesPedigreeFunction    False\nAge                         False\nOutcome                     False\ndtype: bool\nNow let’s do a summary of the statistics!\npima.iloc[:,8:]\n\n\n\n\n\n\n\n\nOutcome\n\n\n\n\n0\n1\n\n\n1\n0\n\n\n2\n1\n\n\n3\n0\n\n\n4\n1\n\n\n...\n...\n\n\n763\n0\n\n\n764\n0\n\n\n765\n0\n\n\n766\n1\n\n\n767\n0\n\n\n\n\n768 rows × 1 columns\n#excludes the outcome column \npima.iloc[:,0:8].describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\nSlay &lt;3\nFrom the results we can make out a few insights: The pregnancy numbers appear to be normally distributed whereas the others seem to be rightly skewed. (The mean and std deviation of pregnancies are more or less the same as opposed to the others). Highest glucose levels is 199, pregnancies 17 and BMI 67.\nNow to the fun part."
  },
  {
    "objectID": "posts/practice_analysis/diabetes_analysis.html#data-visualization",
    "href": "posts/practice_analysis/diabetes_analysis.html#data-visualization",
    "title": "Linear Regression & Data Analysis Basics",
    "section": "Data Visualization",
    "text": "Data Visualization\nPlotting a distribution plot for variable ‘Blood Pressure’.\ndisplot() function which is used to visualize a distribution of the univariate variable. This function uses matplotlib to plot a histogram and fit a kernel density estimate (KDE).\n\nsns.displot(pima['BloodPressure'], kind='kde') \nplt.show()\n\n\n\n\nWhat is the BMI of the person having the highest glucose?\nMax() method finds the highest value.\n\npima[pima['Glucose']==pima['Glucose'].max()]['BMI']\n\n661    42.9\nName: BMI, dtype: float64\n\n\nThe person with the highest glucose value (661) has a bmi of 42.9\nFinding Measures of Central Tendency (the mean,median, and mode) \n\n# mean \nm1 = pima['BMI'].mean() \nprint(m1) \n\n# median \nm2 = pima['BMI'].median() \nprint(m2)\n\n# mode  \nm3 = pima['BMI'].mode()[0] \nprint(m3)\n\n31.992578124999998\n32.0\n32.0\n\n\nStudying the correlation between glucose and insulin using a Scatter Plot.\nA scatter plot is a set of points plotted on horizontal and vertical axes. The scatter plot can be used to study the correlation between the two variables. One can also detect the extreme data points using a scatter plot.\n\nsns.scatterplot(x='Glucose',y='BloodPressure',data=pima)\nplt.show()\n\n\n\n\nNow, to do a linear regression.\nLinear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, meaning that the change in the dependent variable is directly proportional to the change in the independent variable(s). The goal of linear regression is to find the best-fit line that minimizes the distance between the observed data points and the predicted values on the line. This line can then be used to make predictions or understand the impact of the independent variable(s) on the dependent variable.\nThus, linear regressions can only be done with numerical variables (for example, glucose levels or skin thickness).\n\nimport statsmodels.api as sm\n\nHere, we conduct a linear regression using glucose levels as the independent variable (x) and insulin as the dependent variable (y).\n\n\nX = pima['Glucose']\ny = pima['BloodPressure']\n\n# Add a constant column for the intercept term\nX = sm.add_constant(X)\n\n# Create and fit the linear regression model\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Print the summary of the regression results\nprint(results.summary())\n\n# Make predictions\ny_pred = results.predict(X)\n\n# Access the coefficients\nprint(\"Coefficients: \", results.params)\n\n# Plot the data points and the line of best fit\nplt.scatter(X['Glucose'], y, color='blue', label='Data Points')\nplt.plot(X['Glucose'], y_pred, color='red', label='Line of Best Fit')\nplt.xlabel('Glucose')\nplt.ylabel('Blood Pressure')\nplt.title('Linear Regression')\nplt.legend()\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          BloodPressure   R-squared:                       0.023\nModel:                            OLS   Adj. R-squared:                  0.022\nMethod:                 Least Squares   F-statistic:                     18.26\nDate:                Mon, 03 Jul 2023   Prob (F-statistic):           2.17e-05\nTime:                        16:05:01   Log-Likelihood:                -3355.8\nNo. Observations:                 768   AIC:                             6716.\nDf Residuals:                     766   BIC:                             6725.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         57.9377      2.703     21.433      0.000      52.631      63.244\nGlucose        0.0924      0.022      4.273      0.000       0.050       0.135\n==============================================================================\nOmnibus:                      318.039   Durbin-Watson:                   1.961\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1397.465\nSkew:                          -1.903   Prob(JB):                    3.50e-304\nKurtosis:                       8.403   Cond. No.                         489.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nCoefficients:  const      57.937746\nGlucose     0.092376\ndtype: float64"
  },
  {
    "objectID": "posts/practice_analysis/diabetes_analysis.html#what-about-a-logistic-regression",
    "href": "posts/practice_analysis/diabetes_analysis.html#what-about-a-logistic-regression",
    "title": "Linear Regression & Data Analysis Basics",
    "section": "What about a logistic regression?",
    "text": "What about a logistic regression?\nUse a logistic regression when exploring For the diabetes dataset, you would use logistic regression because the values are binary (they are not just 0 and 1). More to come on this in a future blog post!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Basics of LLMs and Their Role in the Field of Biology",
    "section": "",
    "text": "Computational Components (LLMs)\n\nWhat is an LLM?\nA Large Language Model is a type of neural network which uses vast amounts of textual data in order to generate text composed of human language. By identifying patterns and context within the text which is inputted, it is able to respond to questions, create new content, and even make predictions.\n\n\nWhat are the different types of LLMs?\n\nWord grams: These are rudimentary models that predict the next word based on the frequency of word pairs or word bags in the training data. They DO NOT consider context or word order, resulting in less coherent predictions. Text generated using word grams often lacks resemblance to human text.\nCNNs (Convolutional Neural Networks): CNN models analyze text by considering relationships between adjacent words within a fixed window. They can have wide windows using techniques like dilation. While CNNs are good at identifying local patterns, they struggle with capturing long-range dependencies and comprehending complex sentence structures.\nLSTMs (Long Short-Term Memory networks): LSTMs are a variant of Recurrent Neural Networks (RNNs) capable of storing and processing information from earlier parts of a text. They outperform CNNs in understanding context and managing long-range dependencies. However, they still face challenges with complex sentences and long text.\nAttention Mechanisms: Attention mechanisms are not models in themselves, but mechanisms. They allow models to focus on relevant parts of the input when making predictions. These models have multiple attention “heads” that can concentrate on different parts of the previous text. Transformers, a class of language models, implement attention mechanisms.\nLarge Language Models (LLMs): LLMs, such as GPT-3, are transformers trained on vast amounts of data. Their large size facilitates the learning of intricate patterns, relationships, and context within the text. LLMs represent the most advanced language models available and can generate accurate and coherent responses across a wide range of topics.\n\nThe following LLMs use transformer architecture and were breakthroughs in the field:\n\nBERT (Bidirectional Encoder Representations from Transformers): BERT is a series of LLMs introduced by Google. It is trained using masked language modeling and next sentence prediction. BERT understands context from both the left and right sides of the input, making it bidirectional. It has been open-sourced and achieved significant advancements in language understanding.\nGPT (Generative Pretrained Transformer): GPT is a series of LLMs introduced by OpenAI. Unlike BERT, GPT is trained using the traditional language modeling task of autocompletion. It attends only to the left context during training, making it unidirectional. GPT excels in tasks involving text generation and has shown remarkable performance across various domains.\n\nThese types of LLMs vary in their modeling capabilities, with LSTMs and transformers like BERT and GPT being more advanced in understanding context and generating coherent responses. LLMs have significantly evolved, and the latest generation, such as GPT-4, exhibits promising signs of general intelligence.\n\n\nWhat about Data Generation for LLMs for Use in Genetics?\nAdvances in DNA sequencing have allowed us to fully sequence the entire human genome for less than $200. Sequencing-based methods have significantly advanced our ability to measure molecular function. These methods allow for the exposure of crucial molecular information such as chromatin structure, histone modifications, and transcription factor binding to DNA. Short DNA segments with specific properties of interest are isolated and sequenced in experiments to obtain this information. The rapid progress in DNA sequencing technology has outpaced Moore’s law and enabled the measurement of various genetic aspects within biological samples, including gene expression, chromatin accessibility, and histone modifications, often with single-cell or spatial precision.\n\n\nUsing LLMs for Diagnosing Genetic Diseases\nAs mentioned in an earlier post, mutations at splicing sites can completely change which proteins are produced, thus the protein function, resulting in rare genetic diseases. However, using LLMs, predicting splice sites and deducing gene structure becomes simpler and contribute to the diagnosis of rare genetic diseases.\nSpliceAI is a deep residual Convolutional Neural Network (CNN) introduced by the Illumina AI laboratory. It operates by utilizing earlier techniques for language modeling applied to DNA sequences, rather than functioning as a Language Model itself. Its primary purpose is to accurately predict the locations of intron-exon boundaries in the human genome, specifically the donor and acceptor sites. SpliceAI achieved a high precision-recall area under the curve (PR-AUC) score of 0.98, surpassing the previous best score of 0.23.\nOne key feature of SpliceAI is its ability to perform in silico mutational analysis. It can artificially modify DNA positions and determine whether the alterations introduce or eliminate splice sites within 10,000 nucleotides of the mutation. This capability makes SpliceAI valuable for aiding genetic diagnosis, particularly in cases of rare undiagnosed pediatric diseases. By inputting variants of a patient’s DNA into SpliceAI, it can assess the likelihood of altering gene splicing and disrupting gene function. SpliceAI’s high accuracy stems from its deep residual network’s capacity to learn complex biomolecular properties of DNA sequences that guide the splicing machinery to the correct splice sites. It captures and utilizes these previously unknown or imprecisely known properties effectively.\n\n\nPredicting Gene Expression from a DNA Sequence Using LLMs\n\nEnformer is a transformer-based tool and a part of the lineage of language models designed to predict cell type-specific gene expression levels based on the DNA sequence near a gene. It is trained using supervised learning to predict various experimental data types for a given genome region, including chromatin status, histone modifications, transcription factor binding, and gene expression levels. By incorporating attention mechanisms, Enformer can effectively capture correlations between molecular entities across distant regions up to 100,000 nucleotides away.\nWhile Enformer performs reasonably well in predicting gene expression from sequence alone, it currently falls short compared to experimental replicates, correlating at a level of 0.85, with a three-fold higher error rate. However, as more data are incorporated and the model is improved, its performance is expected to enhance. Enformer can also predict changes in gene expression caused by mutations in different individuals and artificially introduced mutations through CRISPR experiments. However, it has limitations in predicting the effects of distal enhancers and determining the direction of the impact of personal variants on gene expression, likely due to insufficient training data.\n\n\n\nEnformers for effective gene expression prediction. Credit: Erik Storrs blog.\n\n\n\n\nFoundation Models\nFoundation models, such as the transformer-based GPT models, are large deep learning architectures that encode a vast amount of knowledge from various sources. They can be fine-tuned for specific tasks, resulting in high-performance systems for different applications. Two recent preprint models in molecular biology are introduced: scGPT and Nucleotide Transformer.\nscGPT is designed for single-cell transcriptomics, chromatin accessibility, and protein abundance. It is trained on single-cell data from 10 million human cells and learns embeddings that provide insights into cellular states and biological pathways. The model is trained to generate data based on gene prompts and cell prompts, predicting genes and their confidence values. scGPT is then fine-tuned for tasks like batch correction, cell annotation, perturbation prediction, multiomics, and pathway prediction.\nNucleotide Transformer focuses on raw DNA sequences and uses the BERT methodology. It tokenizes sequences into k-mers of length 6 and is trained on the reference human genome, diverse human genomes, and genomes of other species. It is applied to 18 downstream tasks, including promoter prediction, splice site prediction, and histone modifications. Predictions are made through probing or computationally inexpensive fine-tuning.\n\n\nWhat the AI Actually Does: Training LLMs in Predicting Gene Expression\nTeach it one-step causality relationships: “if a certain mutation occurs, a specific gene malfunctions. If this gene is under-expressed, other genes in the cascade increase or decrease” (Batzoglou). Ultimately, we want it to learn the complex statistical properties of existing biological systems. Batzoglou states that it can be “learned from triangulating between correlations across modalities such as DNA variation, protein abundance and phenotype (a technique known as Mendelian randomization)”.\nIn all, the deep learning technology is strong enough at this point to take in genomic data and output predictions for gene expression or other biological information. These technologies are continuously being developed, becoming even more powerful, efficient, and precise day-by-day."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My ‘LLM in Biology’ Blog!",
    "section": "",
    "text": "This is the introductory post about the blog!\n\n\n\nCredit: Serafim Batzoglou, “Large Language Models in Biology”. Image from the author, created by Midjourney, prompted by “DNA”.\n\n\nThis blog will ultimately track my progress in learning about Large Language Models (LLMs) and their applications in molecular biology throughout the course of my 8-week program in the Im Lab at the University of Chicago. As I continue to learn biological knowledge and computational skills, I will continue to update this blog with what I have learned."
  },
  {
    "objectID": "posts/neural_networks/index.html",
    "href": "posts/neural_networks/index.html",
    "title": "Neural Networks Code and Explanation",
    "section": "",
    "text": "Import the necessary packages. Also import the iris dataset, which we will be using to train the neural network. The iris dataset is a widely-known, relatively small dataset which can be used to train this simple neural network.\n\nimport torch  #torch is an open source ML library used for creating deep neural networks\nimport torch.nn as nn  \nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.datasets import load_iris #scikit-learn is a data analysis library for machine learning\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder #converts categorical variables into a numerical format that can be used by ML algorithms\nimport matplotlib.pyplot as plt\n\nThe iris dataset contains categorical variables (attributes are sepal length, sepal width, petal length, petal width), which is why we import OneHotEncoder. OneHotEncoder will allow us to preprocess the data from the iris dataset by converting the categorical attributes to numerical formats so that the neural network can understand it.\nBefore getting into the code, here is a conceptual outline of a neural network architecture:\nA neural network has three main components: input layer, hidden layers, output layer.\n\nInput layer: where the neural network receives data represented as numbers.\nHidden layers: layers of the neural network; intermediate processing steps where the model extracts and learns complex patterns and features from the input data. Within the hidden layers, an activation function introduces non-linearities, allowing the network to capture complex relationships and make more sophisticated predictions.\n\nEach hidden layer can capture different levels of abstraction, with earlier layers learning simple features and later layers combining them to learn more complex patterns. The number of hidden layers and the number of neurons within each layer can be adjusted based on the complexity of the problem at hand. By adding more hidden layers, the neural network can learn increasingly abstract representations of the data.\n\nOutput layer: where the final prediction or output based on the computations performed by the hidden layers is provided.\n\nMany people think of neural networks as this mysterious black box which just predicts information. To more concretely understand what’s going on, here is a more detailed explanation what happens when you put input data through a neural network:\nInput data is put into neural networks, and neural networks are a just a collection of nested functions. These functions are defined by parameters (consisting of weights and biases).\nThink of weights as the knobs that control how much attention the neural network should pay to each feature. For example, for a neural network which classifies the fruit in images of various fruits, if the weight for the color feature is high, it means that the network considers color to be more important in the prediction. Conversely, if the weight is low, the network assigns less importance to that feature. During the training phase of a neural network, these weights are adjusted based on the input data and the desired output.\nBiases provide the neural network with the ability to make predictions even when all the input features are zero or missing. In our fruit example, biases can be thought of as an inherent inclination of the neural network to lean towards predicting one fruit over the other, regardless of the input values. They act as an additional input to each neuron in the network and help in adjusting the output of the neuron.\nThe parameters (weights and biases) are stored in tensors. Tensors are PyTorch’s version of arrays and matrices.\nThe entire process of running the input data through each of the NN’s functions to make its best guess about the correct output is known as a forward propagation, or forward pass.\nDuring training, after a forward pass, the NN goes through backward propagation: in backprop, the NN adjusts its parameters proportionate to the error in its guess. This error (known as the “loss”) is the mathematical difference between what the NN predicted and the correct answer (called the “ground truth”). The NN does this by traversing backwards from the output, back into each of the layers, collecting the derivatives of the error with respect to the parameters of the functions (parameters are like settings that control how the NN works).\nOnce the network knows how each parameter affects the error with those derivatives (called “gradients”), it can update the parameters in a way that reduces the error. This updating process is called “gradient descent.” It adjusts the parameters step by step, using the information from the gradients, to make the network better at making predictions.\nBy repeating this process with many examples and adjusting the parameters based on the gradients, the network gradually improves its performance and becomes better at making accurate predictions.\nNow, to the code!\nThe following code defines the neural network:\n\n#defining neural network\n\nclass MyNeural(nn.Module):\n    def __init__(self, input, hidden_layers, output):\n        super().__init__()\n        self.inputLayer = torch.nn.Linear(input, hidden_layers[0])\n        self.hlayers = torch.nn.ModuleDict(\n            {f\"hL{i}\": torch.nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)}\n        )\n        self.outputLayer = torch.nn.Linear(hidden_layers[-1], output)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        x = F.relu(self.inputLayer(x))\n        for name, layer in self.hlayers.items():\n            x = F.relu(layer(x))\n        x = self.outputLayer(x)\n        y = self.sig(x)\n        return y\n\nExplanation of above code, line by line:\n\nWhen using PyTorch, which is a machine learning framework based on the Torch library, like we are here, you need to create a sub-class within the nn.Module and implement your neural network architecture within it. The name of our sub-class, our neural network architecture, is MyNeural.\nInitialize the MyNeural class. The initialization method takes in an input, which is the dimensions of the input layer, dimensions of the hidden layers, and dimensions of the output layer of the neural network. For example, in the iris dataset, the input layer will be 4 because there are 4 attributes: sepal length, sepal width, petal length, petal width (note: do not count the ‘species’ category as one of the attributes because that is what we are trying to predict with this dataset, so it should not be an input).\nThis line calls the initialization method of the parent class (nn.Module). It ensures that the necessary setup is done before defining the architecture of the custom neural network.\nThis line creates the input layer of the neural network using the torch.nn.Linear class. It specifies that the input layer has “input” number of input features and outputs “hidden_layers[0]” number of neurons.\n\nThis linear layer performs a linear transformation on the input data (an oversimplified explanation: takes the input data, factors in weights and biases of the network, and turns the data into the values of the neuron within the network). The .Linear method creates fully connected layers, where each neuron is connected to every neuron in the previous layer (see diagram at start of blog).\n\nThis line creates the hidden layers of the neural network using the torch.nn.Linear class. It creates a ModuleDict object named hlayers, which is a dictionary-like structure that holds the hidden layers. It uses a dictionary comprehension to iterate over the range of len(hidden_layers) - 1 and creates a linear layer for each hidden layer. The keys of the dictionary are formatted as hLi, where i is the index of the hidden layer, and the corresponding value is the linear layer itself. Each linear layer specifies the number of input and output features for the corresponding hidden layer.\nThis line creates the output layer of the neural network using the torch.nn.Linear class. It specifies that the output layer has hidden_layers[-1] number of input features (the output size of the last hidden layer) and output number of output features (neurons).\nSigmoid is an activation function (here we are defining the output function), which will introduce non-linearity and allow the network to capture complex relationships and make more sophisticated predictions. This line creates an instance of the nn.Sigmoid class and assigns it to the variable self.sig.\nDeclares the forward method of the class, which takes x as the input data. The forward methos is used for a “forward pass,” which is the process of inputting data into the neural network, propagating it forwards, through the hidden layers, and producing an output inference/prediction from the output layer.\nThis line applies the ReLU (Rectified Linear Unit) activation function to the input data x after passing it through the self.inputLayer linear layer. The F.relu function is a shorthand for applying the ReLU activation function element-wise. In specific, reLu is an activation function which takes any number you put into it (from the hidden layers), and if any of those numbers is negative, it turns them into positive, and if they are positive, it retains it. The reason for this is to promote efficiency and reduce the computational load of the network by focusing on relevant features and reducing redundant computations\nThis line starts a loop over the hidden layers of the neural network. It iterates through the self.hlayers dictionary, which holds the hidden layers of the network.\nInside the loop, this line applies the ReLU activation function to the intermediate results obtained by passing the data x through each hidden layer (layer). The ReLU activation function introduces non-linearity to the network. The ReLu function needs to be applied after EVERY hidden layer in order to ensure complex patterns are understood by the model, which is why we are iterating through each hidden layer using a for loop.\nAfter the loop, this line applies the output layer (self.outputLayer) to the intermediate results (x). The output layer typically performs a linear transformation on the data without applying an activation function.\nThis line applies the sigmoid activation function (self.s) to the output of the neural network (x). The sigmoid function squeezes the output values between 0 and 1, often used for binary classification or probability estimation. There are many types of activation functions, but Sigmoid is used for predicting probabilities in datasets with attributes that are categorical, like the iris dataset (e.g. sepal width, petal length, etc.)\nThis line returns the final output y from the forward method (produces the output after a forward pass).\n\n\niris = datasets.load_iris()\ndata = iris.data #assigning data (data represents features)\ntarget = iris.target #assigning target (which are class labels)\n\nHere, we the iris dataset.\nWe assign the data part of the Iris dataset to the variable data. iris.data represents the features or input variables. It is a two-dimensional array-like object where each row corresponds to a sample (flower) and each column represents a specific feature (sepal length, sepal width, petal length, petal width). After executing this line, the variable data will hold the feature data from the Iris dataset.\nNext, assign the target part of the Iris dataset. iris.target represents the class labels, which are the species of flower. Labels are like the ground truth or the correct answers that the neural network learns to predict. By providing labels during the training phase, the network learns to associate specific patterns or features in the input data with the corresponding labels. The class labels in the Iris dataset are encoded as integers, where 0 represents setosa, 1 represents versicolor, and 2 represents virginica. After executing this line, the variable target will hold the class label data from the Iris dataset.\n\ntrainD, testD, trainT, testT = train_test_split(data, target, test_size=0.1, random_state=42) \n\ntTrainD = torch.from_numpy(trainD).float() #training data\ntTrainT = torch.from_numpy(trainT).long() #training targets\ntTestD = torch.from_numpy(testD).float() #testing data\ntTestT = torch.from_numpy(testT).long() #testing targets\n\nBefore explaining the above chunk, let’s go through the process of how a neural network is trained and tested.\nAfter loading the dataset, we split the data into train, test, and validation sections (validation is just another test). So, the training will be done on one subset of the data, but the testing will be done on a different subset of the data (but within the same dataset). Doing this allows the model to use the complex relationships and patterns it identified from the training data to predict on different test data.\nIn the first line of code above, we are splitting the dataset into training and test sets (for simplification purposes, we are not splitting it into validation too), and specifying the training and testing targets (species) within the dataset as well. test_size = 0.1 ensures that 10% of the data is allocated for testing. random_state is just some arbitrary parameter that, when set again at the same value, will ensure the same exact random splits in data are used (in case you want to retest the neural network with the same splits of the dataset into test and training groups).\nThe next provided code converts the NumPy arrays representing training and testing data and targets that we made in the first line of code into PyTorch tensors. Tensors are PyTorch’s version of arrays and matrices.\nThis conversion allows for seamless integration with PyTorch and enables further processing, manipulation, and training of neural networks using the converted tensors. The .float() and .long() methods are used to ensure the appropriate data types for the tensors based on the nature of the data (float for input features (numerical data like sepal and petal length) and long for labels or targets (flower species label)).\n\n#creating custom dataset object\n\nclass myDataset():\n    def __init__(self, x, y): #initialize by putting in the dataset, which is 1 million by 4 attributes for iris \n        self.x = x\n        self.y = y\n    def __len__(self): #pytorch will look at the length (the number of rows) in dataset\n        length_ml = self.x.shape[0]\n        return(length_ml)\n    def __getitem__(self, idx): # says how to I source one training item and return it back to you, and the next time you call this function,\n        #it is going to select a different training item (keeping the training items it already used in its memory)\n        return (self.x[idx, :], self.y[idx])\n\nThe first four lines in the above code define a custom dataset class myDataset in PyTorch. The class encapsulates the input features (self.x) and labels (self.y) of the iris dataset into a single object, which helps organize and manage your data in a structured manner, making it easier to work with and reducing the risk of errors or data inconsistencies.\nThe next 3 lines of code (def len(self) …) provide methods to determine the length of the dataset. These methods are called by PyTorch to determine the length of the dataset, i.e., the number of samples in the dataset.\nThe last two lines of code (def getitem(self, idx) …) retrieve individual samples by index. The getitem method allows you to define how individual samples are accessed from the dataset. This customization is valuable when you need to implement specific data preprocessing or transformations before feeding the data into the neural network.\n\ntrainDataset = myDataset(tTrainD, tTrainT)\ntrainDataset\n\n&lt;__main__.myDataset at 0x12edbcd10&gt;\n\n\nBy passing tTrainD (training data) and tTrainT (training targets) as arguments, you are initializing the trainDataset object with the corresponding data and targets in preparation for training.\nExplanation of the process of training neural networks:\nWhen training a neural network, the process of splitting the data into batches is an essential step that enables efficient training. Instead of processing the entire dataset at once, the data is divided into smaller subsets called batches. Each batch contains a fixed number of samples, and the network is updated based on the gradients computed from the predictions and the corresponding targets within the batch.\nThe process of training a neural network with batching typically involves the following steps:\nData Loading: The training dataset is loaded, either as a whole or through a data loader object, which provides access to the data in batches. The data loader takes care of shuffling, batching, and any necessary preprocessing.\nMini-batch Iteration: The training data is divided into mini-batches, each containing a predefined number of samples (specified by the batch size). The network will process one mini-batch at a time.\nForward Pass: For each mini-batch, the input data is fed forward through the network. The network computes predictions for the samples in the mini-batch.\nLoss Computation: The predictions from the forward pass are compared to the corresponding target values (labels) for the samples in the mini-batch. This comparison generates a loss value, which quantifies the error between the predicted outputs and the true targets.\nBackward Pass and Parameter Update: The loss is used to compute gradients with respect to the network parameters. The gradients indicate the direction and magnitude of the updates required to minimize the loss. The gradients are backpropagated through the network using the chain rule of derivatives. The network’s parameters are then updated using an optimization algorithm (e.g., gradient descent or its variants) based on these gradients.\nIteration: Steps 3 to 5 are repeated for each mini-batch in the training data. This process is typically performed for multiple epochs, where an epoch refers to one complete pass through the entire training dataset. The network updates its parameters after each mini-batch, gradually improving its performance over the epochs.\n\ntrain_loader = DataLoader(trainDataset, batch_size=8, shuffle=True)\ntrain_loader\n\n&lt;torch.utils.data.dataloader.DataLoader at 0x14c9dbe90&gt;\n\n\ntrainDataset: This is the dataset object that you want to load using the data loader. It should be an instance of a PyTorch dataset class. We already defined trainDataset st chunk of code using the myDataset custom object we created.\nbatch_size: Specifies the number of samples to load in each batch. In this case, each batch will contain 8 samples.\nshuffle: If set to True, the data loader will shuffle the samples before each epoch (a complete iteration over the dataset). Shuffling the data helps in randomizing the order of the samples and can improve the model’s training performance.\nEssentially, the training examples are split into groups called “batches”. This allows the dataset to be “loaded” into the network, or processed, in more manageable, smaller chunks. Those batches then produce some output values, get backpropagated, and each and every batch individually goes through the NN in each epoch. An epoch is one pass through your entire training data. So if there are 30 epochs, you are training 30 times.\n\n#naming the neural network and setting dimensions of layers\nmyNN = MyNeural(4, [5, 6, 7, 6], 4)\nmyNN\n\nMyNeural(\n  (inputLayer): Linear(in_features=4, out_features=5, bias=True)\n  (hlayers): ModuleDict(\n    (hL0): Linear(in_features=5, out_features=6, bias=True)\n    (hL1): Linear(in_features=6, out_features=7, bias=True)\n    (hL2): Linear(in_features=7, out_features=6, bias=True)\n  )\n  (outputLayer): Linear(in_features=6, out_features=4, bias=True)\n  (sig): Sigmoid()\n)\n\n\nIn the above code, for the neural network model ‘MyNeural’ which we defined earlier:\nWe set 4 input features (4 because of the number attributes of the iris dataset: sepal width, sepal length, petal width, and petal length). The 4 input features produce 5 features (aka 5 neurons) in the first hidden layer, 6 neurons in the second hidden layer, 7 in the third, 6 in the fourth, and finally output 4 features as a prediction. So, the values in the list [5, 6, 7, 6] assign the number of neurons in each hidden layer, and the length of the list corresponds to the number of hidden layers. Here, there are 4 hidden layers. These dimensions for MyNeural are all assigned to myNN – which becomes the name of the specific neural network.\n\nopt = torch.optim.SGD(myNN.parameters(), lr = 0.01)\nlossfunction = nn.CrossEntropyLoss()\n\nBased on the gradients computed by backpropagation, an optimizer is an algorithm or method used to adjust the parameters of the model during the training process in order to minimize the loss (to produce a more accurate prediction). In the above code, we are initializing an optimizer object (opt) of type Stochastic Gradient Descent (SGD). It takes two arguments:\nmyNN.parameters(): This specifies the parameters of your neural network model (myNN) that will be optimized during training. The optimizer will update these parameters based on the computed gradients. lr=0.01: This sets the learning rate for the optimizer. The learning rate determines the step size taken during optimization, influencing how quickly the model learns and converges.\nEven though we used the SGD optimizer here, there other examples of optimizers including Adam, RMSprop, Adagrad, etc., all of which have their own strengths and weaknesses.\nThe loss function calculates the discrepancy between the predicted outputs and the ground truth labels, providing a measure of the model’s performance during training. The CrossEntropyLoss() function from torch.nn is commonly used for multi-class classification tasks, like the classification of species that we are doing with the iris dataset.\n\n#training loop for an NN model\n\nnum_epochs = 1000\ntrain_loss = []\ntest_loss = []\n\nfor epoch in range(num_epochs):\n    myNN.train()\n    running_loss = 0\n    dt_size = 0\n    for i, (batchX, batchY) in enumerate(train_loader): \n        #batchY represents target (actual) labels corresponding to input data batch (batchX)\n        opt.zero_grad()\n        output = myNN(batchX)\n        loss = lossfunction(output, batchY)\n        loss.backward()\n        opt.step()\n        running_loss += loss.item() * batchX.size(0)\n        dt_size += batchX.size(0)\n    train_loss.append(running_loss / dt_size)\n\n    myNN.eval()\n    with torch.no_grad():\n        p = myNN(tTestD) #forward pass on testing data subset\n        l = lossfunction(p, tTestT) #loss calculation\n        test_loss.append(l.item())\n\nThe above code essentially iterates over the training data, performs forward and backward passes, updates the model parameters, and calculates and stores the training and testing loss for each epoch.\nAn explanation of the code, line by line:\n\nnum_epochs = 1000: This variable indicates the number of training epochs, specifying how many times the entire dataset will be iterated (repeatedly ran through the NN) during training.\ntrain_loss and test_loss are empty lists that will store the training and testing loss values for each epoch, respectively, within the for loop later.\nThe loop for epoch in range(num_epochs): iterates over the specified number of epochs.\nmyNN.train() sets the neural network model (myNN) in training mode.\nrunning_loss and dt_size variables are initialized to track the cumulative loss and the total size of the training dataset.\nThe inner loop for i, (batchX, batchY) in enumerate(train_loader): iterates over the batches of data from the training data loader (train_loader). The enumerate(train_loader) part adds an index counter (i) to each batch returned by the train_loader. This means that as you iterate over the batches in the train_loader, you also have access to the index or position of the current batch. The index counter (i) starts from 0 and increments by 1 for each batch in the train_loader. It allows you to keep track of the progress and index of the current batch within the training loop.\nopt.zero_grad() clears the gradients of the optimizer before calculating the new gradients after every batch. This is VERY important to include because during backpropagation, gradients are calculated and stored for each parameter of the model. If the gradients are not cleared, they would accumulate from one iteration to the next. This would result in incorrect gradient values and lead to incorrect updates of the model parameters. Many people forget it!\noutput = myNN(batchX) computes the forward pass of the neural network model on the current batch of inputs (batchX).\nloss = lossfunction(output, batchY) calculates the loss between the predicted outputs and the actual labels (batchY) using the specified loss function (lossfunction).\nloss.backward() performs backpropagation, computing the gradients of the model’s parameters with respect to the loss.\nopt.step() updates the model’s parameters by taking an optimization step using the optimizer (opt).\nrunning_loss += loss.item() * batchX.size(0) and dt_size += batchX.size(0) accumulate the loss and the size of the current batch for later calculation of the average loss.\ntrain_loss.append(running_loss / dt_size) calculates and stores the average training loss for the current epoch.\nmyNN.eval() switches the model to evaluation mode. During the training phase of a neural network, the model undergoes iterations to learn from the training data and update its parameters. However, when it comes to evaluating the model’s performance on a validation or test set, it is important to ensure that the model behaves differently compared to the training phase. This is where the evaluation mode comes into play.\nwith torch.no_grad(): ensures that no gradients are computed during the following evaluation phase.\np = myNN(tTestD) performs the forward pass of the model on the testing dataset (tTestD) to obtain the predicted outputs.\nl = lossfunction(p, tTestT) calculates the loss between the predicted outputs and the testing labels (tTestT).\ntest_loss.append(l.item()) stores the testing loss for the current epoch.\n\n\n#plotting loss with training and testing the NN\n\nplt.plot(train_loss, label='train loss')\nplt.plot(test_loss, label = 'test loss')\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Testing and Training Loss for NN\")\n\nText(0.5, 1.0, 'Testing and Training Loss for NN')\n\n\n\n\n\n\n#testing NN\n\nprint(\"Testing:\")\nopt.zero_grad()\noutput = myNN(tTestD)\nprint(tTestD.shape)\nloss = lossfunction(output, tTestT)\n\nTesting:\ntorch.Size([15, 4])\n\n\nOverall, the code snippet performs a forward pass of the neural network model on the testing data, prints the shape of the testing data, and calculates the loss between the predicted outputs and the target labels. This can be useful for evaluating the performance of the model on the testing data after training.\nPrinting the shape of the testing data is crucial as it allows for data verification, debugging, and input size considerations. It helps ensure that the testing data matches the expected input shape for the neural network model. Additionally, it provides insights into the number of samples and dimensions, which is vital for designing and configuring the model. Checking the data shape ensures that preprocessing steps have been correctly applied.\n\n#analyzing and printing results for one epoch\n\npredictions = []\nfor row in output:\n    if row.max() == row[0]:\n        predictions.append(0)\n    elif row.max() == row[1]:\n        predictions.append(1)\n    else:\n        predictions.append(2)\ntPreds = torch.tensor(predictions).view(15,1)\ntTargets = tTestT.view(15,1)\n\nresult = torch.cat([tPreds,tTargets], dim=1)\nprint(result)\ncorrect = 0\nfor row in result:\n    if row[0] == row[1]:\n        correct += 1\n\nprint(correct)\n\ntensor([[1, 1],\n        [0, 0],\n        [1, 2],\n        [1, 1],\n        [1, 1],\n        [0, 0],\n        [1, 1],\n        [1, 2],\n        [1, 1],\n        [1, 1],\n        [1, 2],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n12\n\n\nThe above code snippet is not necessary but can be helpful to understand the accuracy of the model’s prediction. The code snippet calculates the predicted labels based on the output tensor, compares them with the target labels, and prints the resulting tensor as well as the number of correct predictions."
  },
  {
    "objectID": "posts/command-line-skills/index.html",
    "href": "posts/command-line-skills/index.html",
    "title": "Practicing Working with Terminal",
    "section": "",
    "text": "Working with Terminal ( + GitHub & VSCode)\nThe goal of this 8 week program is to be able to train an LLM to take in a full DNA sequence and predict some biological mechanism, such as gene expression or the effect of transcription factor binding. This can only be accomplished by using a supercomputer with many, many GPUs. These supercomputers will take in the training code from the terminal (or VSCode), which is why it is necessary to learn how to use VSCode and navigate the command line on the terminal of the laptop.\nFirst, you need to ensure that your new file that you are working on is in the correct environment. An environment is kind of like a storage area on your laptop for all your programming tools (e.g. Python)– it’s basically like a folder for everything you need.\nConda: Conda is an environment management tool which ensures that all your code, program, and files for a specific project are in the same environment. It also manages the installation, updating, and removal of packages (e.g. numpy, pandas, etc.). Conda comes with Anaconda and Miniconda, which you can download from the internet and have all the tools you would need to explore, model, and visualize datasets and more.\n\nMake a New Environment\nOpen terminal. Type the following, where “envname” is what you want to call your new environment:\nconda create —name envname\n\n\nActivate Newly Created Environment\nDo this to enter your desired environment. Type the following, where “envname” is your environment name:\nconda activate envname\n\n\nInstalling Tools for Programming in Python within New Environment\nType the following:\nconda install python\n\n\nInstall “pip” before installing packages for Python\npip is is a package management system used to install and manage software packages written in Python. It is a command-line tool that comes bundled with Python installations.\nTo install pip:\nconda install pip\nNote: Make sure you are in the desired environment while doing this.\nYou can check that you are in that desired environment if the environment name is in the parenthesis before the remaining line of code produced by terminal. So, after you activate the environment, it should produce something like this:\n(envname) Your-MacBook-Pro:~ macusername$\nWhen you are in the “normal” default environment, it will look something like this:\n(base) Your-MacBook-Pro:~ macusername$\n\n\nVarious pip commands\nHere are some commonly used pip commands:\n\npip install package_name: Installs a package from PyPI or another source.\n\n**For example, for installing numpy: python -m pip install numpy\nNote: The -m flag is commonly used for running Python scripts that are part of a package or when you want to ensure that the module is executed with the correct environment and dependencies.\n\npip uninstall package_name: Uninstalls a package.\npip list: Lists installed packages.\npip search package_name: Searches PyPI for packages matching the given name.\npip show package_name: Displays information about a specific package.\npip freeze: Generates a requirements.txt file containing a list of installed packages and their versions.\n\n\n\nUsing VSCode with Desired Environment\nVSCode has an integrated terminal that allows you to run commands directly within the editor. However, VSCode’s integrated terminal supports various shells, including PowerShell, Command Prompt (Windows), and Bash (macOS/Linux). So, while coding in VSCode, you need to make sure your code file is in the correct environment for your project. To do this, click on the bottom right of the screen on VSCode (in this example, the tab that says “3.11.3 (‘dlgtools’:conda). dlgtools is the name of my desired project environment. If the name of the environment is not the environment you want to work in, simply click on it and VSCode should open a tab at the top of your screen which says”Select Interpreter” and you can switch into a different environment. Your file will then be stored in this environment, so any packages or programs (like Python) which you plan to use in that file should be in that environment.\n\n\n\nChecking to See If You Have Python\nOnce the terminal is open, type python --version or python3 --version and press Enter.\n\nIf Python is installed, the command will display the version number of Python installed on your system. For example, you might see something like Python 3.9.2.\nIf Python is not installed or if the command is not recognized, you will typically see an error message indicating that the command is not found or recognized. In this case, you’ll need to install Python.\n\n\n\nChecking Your VSCode in Terminal\nIt is usually very difficult to see the output of your code in the integrated terminal of VSCode because it is kind of obscured among the lines about your system, device username, etc. To more clearly see the outputs of your code, you can do the following options:\n\nCheck the code outputs directly in your device’s terminal.\n\n\nOpen terminal.\nMaking sure you are in the desired environment (check the parenthesis), type the following, where file_name is the name of the VSCode file you want to check the code for:\nfrom file_name import *\nThis should clearly produce all the outputs for your code\n\n\nCopy-paste the code into Jupyter Notebook, Google Colab, or some other software with all the packages and tools built in and run the code.\nAdd the following line of code at the very top and very bottom of your coding file to create some space between the outputs within the integrated terminal of VSCode itself. This may not be as helpful to clearly see the code, but it may make a slight difference in visibility.\n\n\nprint(“\\n \\n \\n --------------- \\n \\n \\n”)\n\n\n\n\nFile Navigation in Terminal\nHere are some basic commands in your device’s terminal to make sure you are storing all your files within the desired directory. A directory is basically a type of folder on your device. You need to know which directory you are putting your project files so that you do not lose any important files in the short or long-term.\n\n\nAdditional Navigation Commands in Terminal:\nopen . → opens the directory you are currently in\ncd .. → goes to the parent directory (“steps back”)\nls -a → see all files within the directory (including hidden ones with .git)\ncd \\ → goes to the root directory of the hardware system (the furthest back root)\nBasically, I would start by typing “pwd” in Terminal to determine which directory I am currently in. If you need to move back into an earlier parent folder/directory, type cd ..\nIf you want to move into a further directory, type cd directory_name.\nIf you do not know the name of the directory you want to move further into, or if you do not know if that directory is within your current working directory, type ls to see all directories/files within your current working directory.\nls-a shows all the hidden files as well. Hidden files are files which work in the background of your project, and will begin with a . , which as .git or .nojekyll.\nAfter navigating into the root directory using cd \\, and then going into the desired directory, you can type “echo $PATH” to get the path to get to that directory.\n\n\n\nGithub/Git Commands in Terminal\n\nCloning on GitHub\nCloning refers to creating a local copy of an entire repository, including all its files, commit history, branches, and configuration. When you clone a repository, you create an identical copy on your local machine. This allows you to work with the project, make changes, commit them, and push them back to the remote repository. Cloning is typically used when you want to contribute to a project or work on your own project locally.\nTo clone a repository from GitHub, follow these steps:\n\nOpen the GitHub repository page in your web browser.\nClick on the “Code” button, located near the top-right corner of the repository page.\nClick on the clipboard icon to copy the repository’s URL. Alternatively, you can click on the “Download ZIP” button to download a compressed version of the repository instead of cloning it with Git.\nOpen your terminal or Git Bash (if you’re on Windows).\nNavigate to the directory where you want to clone the repository. You can use the cd command to change directories.\nOnce you’re in the desired directory, use the following command to clone the repository:\n\ngit clone &lt;paste_repository_URL&gt;\n\n\nAfter making changes to local files on your device, you want to sync those changes to the remote, master directory on Github. Git is a tools which allows this syncing. Carry out the following steps to do so:\n\ngit add * → add your changes\ngit status → check you are adding the files/directories you want\ngit commit -m ‘message’ → add a message\ngit push → update the master directory with your work\n\n\n\nAnother option – “pulling” in Github:\nPulling is the opposite of pushing: it’s what you do when the remote, master directory (maybe owned by someone other than you) has changes and you want to update your local directory with those changes. Typically used in collaborative files.\n\ngit pull → update your local directory with the master (remote) directory\n\n\n\nWhen would you pull?\nThe `git pull` command is used to update your local repository with the latest changes from a remote repository, typically the one you cloned from. It incorporates changes made by others and brings your local copy up to date.\nYou would use `git pull` in a few different scenarios:\n1. **Working on a shared project**: If you are collaborating with other people on a project, they might have made changes to the remote repository that you want to sync with. Running `git pull` will fetch those changes and merge them into your local branch.\n2. **Staying up to date**: Even if you’re not collaborating with others, it’s good practice to regularly update your local repository with the latest changes from the remote repository. This ensures that you have the most recent version of the code and can avoid conflicts when you eventually push your own changes.\n3. **Resolving conflicts**: Sometimes, when you pull changes from the remote repository, there might be conflicts between your local changes and the incoming changes. For example, if someone edited the same lines which you edited, and now there are different versions of the same file in the repo. In such cases, Git will notify you of the conflicts and provide an opportunity to resolve them manually.\nIt’s important to note that before running `git pull`, you should commit your local changes to avoid conflicts. If you have uncommitted changes, Git may ask you to stash or commit them before pulling.\n\n\nForking in Github:\nForking a file in GitHub is like making a personal copy of someone else’s file or project. When you fork a file, you create your own version of it that you can modify and make changes to without affecting the original file.\nHere’s a simple analogy: Imagine you have a friend who has a really cool drawing. You want to add your own touches and modifications to that drawing, but you don’t want to mess up your friend’s original. So, what you do is make a photocopy of the drawing and work on that copy. This way, you can freely experiment and make changes without worrying about ruining the original.\nIn GitHub, forking is similar. If you find a file or project in someone else’s repository that you want to modify or contribute to, you can fork it to create your own personal copy of that repository. This copy will be stored in your GitHub account, and you can make changes without affecting the original file or the owner’s repository.\nOnce you’ve made the desired changes to your forked repository, you can choose to share those changes with the original owner through a process called a pull request. This allows the owner to review your changes and decide whether to incorporate them into the original file or project.\n\n\nHow to fork a file on Github?\nTo fork a repository in GitHub, including all its files, branches, and commit history, follow these steps:\n\nOpen your web browser and go to the GitHub repository page that contains the file you want to fork.\nIn the top-right corner of the repository page, click on the “Fork” button.\nGitHub will prompt you to select where you want to fork the repository. Choose your user account or any organization you belong to. Click on the appropriate option.\nGitHub will then create a copy of the repository under your account or organization. Once the forking process is complete, you will be redirected to the forked repository’s page.\n\nNow you have successfully forked the entire repository, not just an individual file. You will have a separate copy of the repository in your GitHub account. This copy will include all the files, branches, and commit history present in the original repository.\nYou can make changes to the files, add new features, fix bugs, or experiment with the forked repository as you see fit. You can commit and push changes to the forked repository without affecting the original repository"
  },
  {
    "objectID": "posts/DNA-locus-extraction/index.html",
    "href": "posts/DNA-locus-extraction/index.html",
    "title": "How to Extract a DNA Sequence Centered at a Specific Locus",
    "section": "",
    "text": "Using a DNA sequence, researchers often want to predict something (e.g. gene expression, TF binding, histone modification, etc) at a specific locus (“location” on the genome) relevant to a particular trait or phenotype. For example, my project will be examining histone modifications at the 17q locus, which is the most significant locus for asthma (the part of the genome which is most influential, genetically, on the asthma condition). To accomplish this, it is important to know how extract from a full DNA sequence (which is 3 billion nucleotides long) the relevant locus.\nYou can use the following function to extract a DNA sequence at a specific locus center (in Python):\n\nclass FastaStringExtractor:\n    def __init__(self, fasta_file):\n        import pyfaidx\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    def extract(self, interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        import kipoiseq\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = kipoiseq.Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                            trimmed_interval.start + 1,\n                                            trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n    def close(self):\n        return self.fasta.close()\n\nCredits for the function: Saideep Gona @ Im Lab at UChicago\nYou will need the pyfaidx and kipoiseq packages to use the function:\n\n!pip install pyfaidx \n!pip install kipoiseq==0.5.2\n\nTo use this function (after you have defined it), first create an instance of the FastaStringExtractor object, providing a fasta file you want to use. This fasta file is going to contain the full DNA sequence which you want to extract from:\n\nfasta_extractor_object = FastaStringExtractor(fasta_file)\n#you should have defined the path of your fasta file and assigned it to some variable before this\n\nNameError: name 'fasta_file' is not defined\n\n\nThen, create an interval you want to extract the sequence from using kipoiseq (“.resize” will resize the interval you provide to a length provided by “sequence_length”. The new interval will be centered on the “start”,“end” you provided originally):\n\ntarget_interval = kipoiseq.Interval(chrom,start,end).resize(sequence_length)\n\nNameError: name 'kipoiseq' is not defined\n\n\nThen, create an interval you want to extract the sequence from using kipoiseq (“.resize” will resize the interval you provide to a length provided by “sequence_length”. The new interval will be centered on the “start”,“end” you provided originally):\n\ntarget_interval = kipoiseq.Interval(chrom,start,end).resize(sequence_length)\n\nFinally, extract the sequence using the extractor object like this:\n\nextracted_sequence = fasta_extractor_object.extract(target_interval)\n\nThat’s it!"
  },
  {
    "objectID": "posts/query-encode/encode-update.html",
    "href": "posts/query-encode/encode-update.html",
    "title": "GPT in Genomics",
    "section": "",
    "text": "%pip install requests\n\nRequirement already satisfied: requests in /Users/satyadonapati/anaconda3/envs/dlgtools/lib/python3.11/site-packages (2.31.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/satyadonapati/anaconda3/envs/dlgtools/lib/python3.11/site-packages (from requests) (3.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/satyadonapati/anaconda3/envs/dlgtools/lib/python3.11/site-packages (from requests) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/satyadonapati/anaconda3/envs/dlgtools/lib/python3.11/site-packages (from requests) (1.26.16)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/satyadonapati/anaconda3/envs/dlgtools/lib/python3.11/site-packages (from requests) (2023.5.7)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport requests, json"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Language Models in Molecular Biology",
    "section": "",
    "text": "this is a title\n\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nsrusti\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression & Data Analysis Basics\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nHow to Extract a DNA Sequence Centered at a Specific Locus\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nNeural Networks Code and Explanation\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nSrusti\n\n\n\n\n\n\n  \n\n\n\n\nGWAS Code\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nPracticing Working with Terminal\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nThe Biology to be Explored in LLMs\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nBasics of LLMs and Their Role in the Field of Biology\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My ‘LLM in Biology’ Blog!\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is essentially a project of progress reports on biological knowledge and computational skills that I have acquired over my 8 weeks in the Im Lab during Summer 2023 for the GPT in Genomics Project."
  }
]