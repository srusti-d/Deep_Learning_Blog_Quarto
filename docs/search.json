[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "The Biology to be Explored in LLMs",
    "section": "",
    "text": "This post will be summarizing key points about molecular biology which I have learned from the article “Large Language Models in Molecular Biology” by Serafim Batzoglou.\nIn an attempt to further explore LLMs to prepare for my upcoming project regarding LLMs, the following summary of the biological information in the above article was generated by prompting ChatGPT and edited by me.\n\nCellular and Molecular Biology Components\n\nCentral Dogma\nThe central dogma of molecular biology explains how genetic information flows within living organisms. It states that DNA, which is housed in the nucleus of every cell, is the source of this genetic information. Human DNA consists of approximately 3 billion nucleotides organized into 23 chromosomes, with 22 being autosomes and one being a sex chromosome (X or Y). Each person inherits two nearly identical copies of the human genome, one from each parent. The genetic material from both parents is retained in the nucleus of each of the roughly 30 trillion cells in the human body. The genome contains about 20,000 genes responsible for protein synthesis, with only about 1% of the genome coding for proteins. The remaining portions of the genome include regions that control gene expression, regions within genes that do not code for proteins, regions contributing to DNA structure, and “junk” regions of self-replicating DNA.\nProtein synthesis, a fundamental process in molecular biology, involves three main steps: transcription, splicing, and translation. During transcription, a DNA segment serving as a gene template is copied into messenger RNA (mRNA). The mRNA molecule undergoes splicing, where certain segments, called introns, are removed, and the remaining segments, called exons, are joined together to form mature mRNA. Splicing is crucial in higher organisms because it allows a single gene to produce multiple protein variants by assembling different combinations of exons. The mRNA is then transported to the ribosome, where translation occurs. During translation, the mRNA sequence is decoded into amino acids, which are the building blocks of proteins. These amino acids are linked together to form a protein sequence, which folds into a functional three-dimensional structure. Proteins play essential roles in various biological processes, providing structural components, catalyzing reactions as enzymes, and facilitating communication and transportation within cells.\n\n\nGene Regulation\nGene regulation is a complex process that controls when, where, and in what quantity genes are expressed in cells. It ensures the timely production of the right proteins in appropriate amounts. Gene regulation occurs at different levels, involving chromatin structure, chemical modifications, and the action of transcription factors. Transcription factors are proteins that bind to specific DNA sequences and influence the recruitment of RNA polymerase, the enzyme responsible for mRNA synthesis. They help regulate the expression of target genes to ensure they are appropriately expressed in response to signals.\nPromoters and enhancers are DNA regions that contribute to gene expression control, with promoters located adjacent to gene starts and enhancers situated within introns or between genes, further downstream in the DNA. Chromatin structure, formed by DNA wrapping around histone proteins, determines which DNA regions are accessible for gene expression. Chemical modifications of histones and DNA, such as acetylation, methylation, and DNA methylation, can influence chromatin structure and gene expression. Gene regulation is specific to each type of cell - some cells have certain genes expressed while other cells have different genes expressed. This is what allows cells to have specialized functions.\nThe flow of genetic information is traditionally described as unidirectional: DNA to RNA to protein. However, there are exceptions to this rule. Reverse transcription allows RNA to be converted back into DNA, as seen in retroviruses like HIV. DNA can also be transcribed into different types of RNA, such as transfer RNA (tRNA) and ribosomal RNA (rRNA), adding complexity to genetic information flow.\n\n\nEpigenetic Mechanisms\nEpigenetic mechanisms, including DNA methylation and histone modifications, play a role in gene regulation and can be inherited. DNA methylation is a chemical modification where methyl is added to the DNA molecule, usually at specific cytosine bases. Methylation influences gene expression by affecting the binding of transcription factors and the chromatin structure. Chromatin must be unfolded for gene expression, so by making the chromatin more compact, methylation makes transcription more difficult (affects gene accessibility).\nDNA variation contributes to the diversity and heritability of traits among individuals. DNA variants are introduced primarily through mutations between the genomes of parents and germline genomes passed on to offspring. Deleterious variants tend to be eliminated from the population over time through natural selection. Genetic variations common in humans are typically benign or contribute to diseases that manifest later in life. Some rare mutations can affect the splicing sites (the boundaries where genes are spliced). As a result, they can cause the production of a completely different protein sequence, thus different protein function. This is why they contribute to 10% of rare genetic diseases.\nSo, predicting splice sites and determining gene structure is important to diagnose genetic diseases."
  },
  {
    "objectID": "posts/population-stratification/index.html",
    "href": "posts/population-stratification/index.html",
    "title": "GWAS Code",
    "section": "",
    "text": "GWAS is a genome-wide association study. It observes the differences in genes by a scanning the genomes of a large number of individuals to identify genetic markers, such as single nucleotide polymorphisms (SNPs), that are associated with a specific trait or condition.\nThe process of conducting a GWAS typically involves comparing the genomes of individuals with a particular trait or disease to those without the trait or disease (known as the reference genome – a genome widely agreed upon scientists in the field to be the “control” genome in which other genomes are compared). By identifying genetic markers that are more frequently present in the group with the trait or disease, researchers can infer potential associations between specific genetic variants and the trait of interest.These associations between genes and traits includes traits associated with a disease or condition.\nHowever, it is important to note that GWAS findings often identify statistical associations between genetic markers and traits, rather than direct causal relationships. The inability to identify a direct causal relationship between genetic markers and traits in a GWAS is because most genetic variants do not change the coding of proteins (which carry out the cell’s functions, which can result in a specific trait).\nFurther research and functional studies are typically required to validate and understand the biological significance of these associations. However, GWAS is a good first-step in identifying genes highly associated with a trait in order to pursue next steps of causal identification."
  },
  {
    "objectID": "posts/population-stratification/index.html#genome-wide-association-study-gwas",
    "href": "posts/population-stratification/index.html#genome-wide-association-study-gwas",
    "title": "GWAS Code",
    "section": "",
    "text": "GWAS is a genome-wide association study. It observes the differences in genes by a scanning the genomes of a large number of individuals to identify genetic markers, such as single nucleotide polymorphisms (SNPs), that are associated with a specific trait or condition.\nThe process of conducting a GWAS typically involves comparing the genomes of individuals with a particular trait or disease to those without the trait or disease (known as the reference genome – a genome widely agreed upon scientists in the field to be the “control” genome in which other genomes are compared). By identifying genetic markers that are more frequently present in the group with the trait or disease, researchers can infer potential associations between specific genetic variants and the trait of interest.These associations between genes and traits includes traits associated with a disease or condition.\nHowever, it is important to note that GWAS findings often identify statistical associations between genetic markers and traits, rather than direct causal relationships. The inability to identify a direct causal relationship between genetic markers and traits in a GWAS is because most genetic variants do not change the coding of proteins (which carry out the cell’s functions, which can result in a specific trait).\nFurther research and functional studies are typically required to validate and understand the biological significance of these associations. However, GWAS is a good first-step in identifying genes highly associated with a trait in order to pursue next steps of causal identification."
  },
  {
    "objectID": "posts/population-stratification/index.html#exploring-gwas-using-a-smaller-genomic-dataset.",
    "href": "posts/population-stratification/index.html#exploring-gwas-using-a-smaller-genomic-dataset.",
    "title": "GWAS Code",
    "section": "Exploring GWAS using a smaller genomic dataset.",
    "text": "Exploring GWAS using a smaller genomic dataset.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(devtools)\n\nLoading required package: usethis\n\ndevtools::source_gist(\"38431b74c6c0bf90c12f\")\n\nℹ Sourcing gist \"38431b74c6c0bf90c12f\"\nℹ SHA-1 hash of file is \"7534a04652d34154de13d2fa2ac042adb0d1f564\"\n\n\n\n#downloading data\n\nif(!file.exists(glue::glue(\"~/Downloads/analysis_population_structure.tgz\")))\n{\n  system(glue::glue(\"wget -O ~/Downloads/analysis_population_structure.tgz https://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\"))\n  ## tar -xf file_name.tar.gz --directory /target/directory\n  system(glue::glue(\"tar xvf ~/Downloads/analysis_population_structure.tgz --directory ~/Downloads/\")) \n}\n\nWarning in system(glue::glue(\"wget -O\n~/Downloads/analysis_population_structure.tgz\nhttps://uchicago.box.com/shared/static/zv1jyevq01mt130ishx25sgb1agdu8lj.tgz\")):\nerror in running command\n\nwork.dir =\"~/Downloads/analysis_population_structure/\"\n\n\n#testing Hardy-Weinberg Equilibrium with population structure\npopinfo = read_tsv(paste0(work.dir,\"relationships_w_pops_051208.txt\"))\n\nRows: 1301 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): FID, IID, dad, mom, population\ndbl (2): sex, pheno\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npopinfo %&gt;% count(population)\n\n# A tibble: 11 × 2\n   population     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 ASW           90\n 2 CEU          180\n 3 CHB           90\n 4 CHD          100\n 5 GIH          100\n 6 JPT           91\n 7 LWK          100\n 8 MEX           90\n 9 MKK          180\n10 TSI          100\n11 YRI          180\n\nsamdata = read_tsv(paste0(work.dir,\"phase3_corrected.psam\"),guess_max = 2500)  \n\nRows: 2504 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): #IID, PAT, MAT, SuperPop, Population\ndbl (1): SEX\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsuperpop = samdata %&gt;% select(SuperPop,Population) %&gt;% unique()\nsuperpop = rbind(superpop, data.frame(SuperPop=c(\"EAS\",\"HIS\",\"AFR\"),Population=c(\"CHD\",\"MEX\",\"MKK\")))\n\n\n## what happens if we calculate HWE with this mixed population?\nif(!file.exists(glue::glue(\"{work.dir}output/allhwe.hwe\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --out {work.dir}output/allhwe\"))\nallhwe = read.table(glue::glue(\"{work.dir}output/allhwe.hwe\"),header=TRUE,as.is=TRUE)\nhist(allhwe$P)\n\n\n\n\n\nqqunif(allhwe$P,main='HWE HapMap3 All Pop')\n\nWarning in qqunif(allhwe$P, main = \"HWE HapMap3 All Pop\"): thresholding p to\n1e-30\n\n\n\n\n\n\npop = \"CHB\"\npop = \"CEU\"\npop = \"YRI\"\nfor(pop in c(\"CHB\",\"CEU\",\"YRI\"))\n{\n  ## what if we calculate with single population?\n  popinfo %&gt;% filter(population==pop) %&gt;%\n    write_tsv(path=glue::glue(\"{work.dir}{pop}.fam\") )\n  if(!file.exists(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\")))\n  system(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --hardy --keep {work.dir}{pop}.fam --out {work.dir}output/hwe-{pop}\"))\n  pophwe = read.table(glue::glue(\"{work.dir}output/hwe-{pop}.hwe\"),header=TRUE,as.is=TRUE)\n  hist(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n  qqunif(pophwe$P,main=glue::glue(\"HWE {pop} and founders only\"))\n}\n\nWarning: The `path` argument of `write_tsv()` is deprecated as of readr 1.4.0.\nℹ Please use the `file` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#effect of population stratification on GWAS\n## read igrowth\nigrowth = read_tsv(\"https://raw.githubusercontent.com/hakyimlab/igrowth/master/rawgrowth.txt\", show_col_types = FALSE)\n\n\n## add FID to igrowth file\nigrowth = popinfo %&gt;% select(-pheno) %&gt;% inner_join(igrowth %&gt;% select(IID,growth), by=c(\"IID\"=\"IID\"))\nwrite_tsv(igrowth,path=glue::glue(\"{work.dir}igrowth.pheno\"))\nigrowth %&gt;% ggplot(aes(population,growth)) + geom_violin(aes(fill=population)) + geom_boxplot(width=0.2,col='black',fill='gray',alpha=.8) + theme_bw(base_size = 15)\n\nWarning: Removed 130 rows containing non-finite values (`stat_ydensity()`).\n\n\nWarning: Removed 130 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\nsummary( lm(growth~population,data=igrowth) )\n\n\nCall:\nlm(formula = growth ~ population, data = igrowth)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-58821 -18093  -2242  15896  98760 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    73080.8      938.2  77.894  &lt; 2e-16 ***\npopulationCEU  -2190.1     1175.4  -1.863   0.0625 .  \npopulationCHB   9053.1     2043.9   4.429 9.73e-06 ***\npopulationJPT   3476.8     2034.8   1.709   0.0876 .  \npopulationYRI  -7985.2     1137.2  -7.022 2.61e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24160 on 3591 degrees of freedom\n  (130 observations deleted due to missingness)\nMultiple R-squared:  0.0345,    Adjusted R-squared:  0.03342 \nF-statistic: 32.08 on 4 and 3591 DF,  p-value: &lt; 2.2e-16\n\n\n\n## run plink linear regression only if it hasn't been run already\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --maf 0.05 --out {work.dir}output/igrowth\"))\nigrowth.assoc = read.table(glue::glue(\"{work.dir}output/igrowth.assoc.linear\"),header=T,as.is=T)\nhist(igrowth.assoc$P)\n\n\n\n\n\nqqunif(igrowth.assoc$P)\n\n\n\n\n\n## install.packages(\"qqman\")\nlibrary(qqman)\n\n\n\n\nFor example usage please run: vignette('qqman')\n\n\n\n\n\nCitation appreciated but not required:\n\n\nTurner, (2018). qqman: an R package for visualizing GWAS results using Q-Q and manhattan plots. Journal of Open Source Software, 3(25), 731, https://doi.org/10.21105/joss.00731.\n\n\n\n\nmanhattan(igrowth.assoc, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\" )\n\n\n\n\n\n## generate PCs using plink\nif(!file.exists(glue::glue(\"{work.dir}output/pca.eigenvec\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --pca --out {work.dir}output/pca\"))\n## read plink calculated PCs\npcplink = read.table(glue::glue(\"{work.dir}output/pca.eigenvec\"),header=F, as.is=T)\nnames(pcplink) = c(\"FID\",\"IID\",paste0(\"PC\", c(1:(ncol(pcplink)-2))) )\npcplink = popinfo %&gt;% left_join(superpop,by=c(\"population\"=\"Population\")) %&gt;% inner_join(pcplink, by=c(\"FID\"=\"FID\", \"IID\"=\"IID\"))\n## plot PC1 vs PC2\npcplink %&gt;% ggplot(aes(PC1,PC2,col=population,shape=SuperPop)) + geom_point(size=3,alpha=.7) + theme_bw(base_size = 15)\n\n\n\n\n\n#running igrowth GWAS using PCs\n\nif(!file.exists(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\")))\nsystem(glue::glue(\"~/bin/plink --bfile {work.dir}hapmapch22 --linear --pheno {work.dir}igrowth.pheno --pheno-name growth --covar {work.dir}output/pca.eigenvec --covar-number 1-4 --hide-covar --maf 0.05 --out {work.dir}output/igrowth-adjPC\"))\nigrowth.adjusted.assoc = read.table(glue::glue(\"{work.dir}output/igrowth-adjPC.assoc.linear\"),header=T,as.is=T)\n##indadd = igrowth.adjusted.assoc$TEST==\"ADD\"\ntitulo = \"igrowh association adjusted for PCs\"\nhist(igrowth.adjusted.assoc$P,main=titulo)\n\n\n\n\n\nqqunif(igrowth.adjusted.assoc$P,main=titulo)"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Basics of LLMs and Their Role in the Field of Biology",
    "section": "",
    "text": "Computational Components (LLMs)\n\nWhat is an LLM?\nA Large Language Model is a type of neural network which uses vast amounts of textual data in order to generate text composed of human language. By identifying patterns and context within the text which is inputted, it is able to respond to questions, create new content, and even make predictions.\n\n\nWhat are the different types of LLMs?\n\nWord grams: These are rudimentary models that predict the next word based on the frequency of word pairs or word bags in the training data. They DO NOT consider context or word order, resulting in less coherent predictions. Text generated using word grams often lacks resemblance to human text.\nCNNs (Convolutional Neural Networks): CNN models analyze text by considering relationships between adjacent words within a fixed window. They can have wide windows using techniques like dilation. While CNNs are good at identifying local patterns, they struggle with capturing long-range dependencies and comprehending complex sentence structures.\nLSTMs (Long Short-Term Memory networks): LSTMs are a variant of Recurrent Neural Networks (RNNs) capable of storing and processing information from earlier parts of a text. They outperform CNNs in understanding context and managing long-range dependencies. However, they still face challenges with complex sentences and long text.\nAttention Mechanisms: Attention mechanisms are not models in themselves, but mechanisms. They allow models to focus on relevant parts of the input when making predictions. These models have multiple attention “heads” that can concentrate on different parts of the previous text. Transformers, a class of language models, implement attention mechanisms.\nLarge Language Models (LLMs): LLMs, such as GPT-3, are transformers trained on vast amounts of data. Their large size facilitates the learning of intricate patterns, relationships, and context within the text. LLMs represent the most advanced language models available and can generate accurate and coherent responses across a wide range of topics.\n\nThe following LLMs use transformer architecture and were breakthroughs in the field:\n\nBERT (Bidirectional Encoder Representations from Transformers): BERT is a series of LLMs introduced by Google. It is trained using masked language modeling and next sentence prediction. BERT understands context from both the left and right sides of the input, making it bidirectional. It has been open-sourced and achieved significant advancements in language understanding.\nGPT (Generative Pretrained Transformer): GPT is a series of LLMs introduced by OpenAI. Unlike BERT, GPT is trained using the traditional language modeling task of autocompletion. It attends only to the left context during training, making it unidirectional. GPT excels in tasks involving text generation and has shown remarkable performance across various domains.\n\nThese types of LLMs vary in their modeling capabilities, with LSTMs and transformers like BERT and GPT being more advanced in understanding context and generating coherent responses. LLMs have significantly evolved, and the latest generation, such as GPT-4, exhibits promising signs of general intelligence.\n\n\nWhat about Data Generation for LLMs for Use in Genetics?\nAdvances in DNA sequencing have allowed us to fully sequence the entire human genome for less than $200. Sequencing-based methods have significantly advanced our ability to measure molecular function. These methods allow for the exposure of crucial molecular information such as chromatin structure, histone modifications, and transcription factor binding to DNA. Short DNA segments with specific properties of interest are isolated and sequenced in experiments to obtain this information. The rapid progress in DNA sequencing technology has outpaced Moore’s law and enabled the measurement of various genetic aspects within biological samples, including gene expression, chromatin accessibility, and histone modifications, often with single-cell or spatial precision.\n\n\nUsing LLMs for Diagnosing Genetic Diseases\nAs mentioned in an earlier post, mutations at splicing sites can completely change which proteins are produced, thus the protein function, resulting in rare genetic diseases. However, using LLMs, predicting splice sites and deducing gene structure becomes simpler and contribute to the diagnosis of rare genetic diseases.\nSpliceAI is a deep residual Convolutional Neural Network (CNN) introduced by the Illumina AI laboratory. It operates by utilizing earlier techniques for language modeling applied to DNA sequences, rather than functioning as a Language Model itself. Its primary purpose is to accurately predict the locations of intron-exon boundaries in the human genome, specifically the donor and acceptor sites. SpliceAI achieved a high precision-recall area under the curve (PR-AUC) score of 0.98, surpassing the previous best score of 0.23.\nOne key feature of SpliceAI is its ability to perform in silico mutational analysis. It can artificially modify DNA positions and determine whether the alterations introduce or eliminate splice sites within 10,000 nucleotides of the mutation. This capability makes SpliceAI valuable for aiding genetic diagnosis, particularly in cases of rare undiagnosed pediatric diseases. By inputting variants of a patient’s DNA into SpliceAI, it can assess the likelihood of altering gene splicing and disrupting gene function. SpliceAI’s high accuracy stems from its deep residual network’s capacity to learn complex biomolecular properties of DNA sequences that guide the splicing machinery to the correct splice sites. It captures and utilizes these previously unknown or imprecisely known properties effectively.\n\n\nPredicting Gene Expression from a DNA Sequence Using LLMs\n\nEnformer is a transformer-based tool and a part of the lineage of language models designed to predict cell type-specific gene expression levels based on the DNA sequence near a gene. It is trained using supervised learning to predict various experimental data types for a given genome region, including chromatin status, histone modifications, transcription factor binding, and gene expression levels. By incorporating attention mechanisms, Enformer can effectively capture correlations between molecular entities across distant regions up to 100,000 nucleotides away.\nWhile Enformer performs reasonably well in predicting gene expression from sequence alone, it currently falls short compared to experimental replicates, correlating at a level of 0.85, with a three-fold higher error rate. However, as more data are incorporated and the model is improved, its performance is expected to enhance. Enformer can also predict changes in gene expression caused by mutations in different individuals and artificially introduced mutations through CRISPR experiments. However, it has limitations in predicting the effects of distal enhancers and determining the direction of the impact of personal variants on gene expression, likely due to insufficient training data.\n\n\n\nEnformers for effective gene expression prediction. Credit: Erik Storrs blog.\n\n\n\n\nFoundation Models\nFoundation models, such as the transformer-based GPT models, are large deep learning architectures that encode a vast amount of knowledge from various sources. They can be fine-tuned for specific tasks, resulting in high-performance systems for different applications. Two recent preprint models in molecular biology are introduced: scGPT and Nucleotide Transformer.\nscGPT is designed for single-cell transcriptomics, chromatin accessibility, and protein abundance. It is trained on single-cell data from 10 million human cells and learns embeddings that provide insights into cellular states and biological pathways. The model is trained to generate data based on gene prompts and cell prompts, predicting genes and their confidence values. scGPT is then fine-tuned for tasks like batch correction, cell annotation, perturbation prediction, multiomics, and pathway prediction.\nNucleotide Transformer focuses on raw DNA sequences and uses the BERT methodology. It tokenizes sequences into k-mers of length 6 and is trained on the reference human genome, diverse human genomes, and genomes of other species. It is applied to 18 downstream tasks, including promoter prediction, splice site prediction, and histone modifications. Predictions are made through probing or computationally inexpensive fine-tuning.\n\n\nWhat the AI Actually Does: Training LLMs in Predicting Gene Expression\nTeach it one-step causality relationships: “if a certain mutation occurs, a specific gene malfunctions. If this gene is under-expressed, other genes in the cascade increase or decrease” (Batzoglou). Ultimately, we want it to learn the complex statistical properties of existing biological systems. Batzoglou states that it can be “learned from triangulating between correlations across modalities such as DNA variation, protein abundance and phenotype (a technique known as Mendelian randomization)”.\nIn all, the deep learning technology is strong enough at this point to take in genomic data and output predictions for gene expression or other biological information. These technologies are continuously being developed, becoming even more powerful, efficient, and precise day-by-day."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My ‘LLM in Biology’ Blog!",
    "section": "",
    "text": "This is the introductory post about the blog!\n\n\n\nCredit: Serafim Batzoglou, “Large Language Models in Biology”. Image from the author, created by Midjourney, prompted by “DNA”.\n\n\nThis blog will ultimately track my progress in learning about Large Language Models (LLMs) and their applications in molecular biology throughout the course of my 8-week program in the Im Lab at the University of Chicago. As I continue to learn biological knowledge and computational skills, I will continue to update this blog with what I have learned."
  },
  {
    "objectID": "posts/neural_networks/index.html",
    "href": "posts/neural_networks/index.html",
    "title": "Neural Networks Code and Explanation",
    "section": "",
    "text": "Import the necessary packages. Also import the iris dataset, which we will be using to train the neural network. The iris dataset is a widely-known, relatively small dataset which can be used to train this simple neural network.\n\nimport torch  #torch is an open source ML library used for creating deep neural networks\nimport torch.nn as nn  \nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.datasets import load_iris #scikit-learn is a data analysis library for machine learning\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder #converts categorical variables into a numerical format that can be used by ML algorithms\nimport matplotlib.pyplot as plt\n\nThe iris dataset contains categorical variables (attributes are sepal length, sepal width, petal length, petal width), which is why we import OneHotEncoder. OneHotEncoder will allow us to preprocess the data from the iris dataset by converting the categorical attributes to numerical formats so that the neural network can understand it.\nBefore getting into the code, here is a conceptual outline of a neural network architecture:\nA neural network has three main components: input layer, hidden layers, output layer.\n\nInput layer: where the neural network receives data represented as numbers.\nHidden layers: layers of the neural network; intermediate processing steps where the model extracts and learns complex patterns and features from the input data. Within the hidden layers, an activation function introduces non-linearities, allowing the network to capture complex relationships and make more sophisticated predictions.\n\nEach hidden layer can capture different levels of abstraction, with earlier layers learning simple features and later layers combining them to learn more complex patterns. The number of hidden layers and the number of neurons within each layer can be adjusted based on the complexity of the problem at hand. By adding more hidden layers, the neural network can learn increasingly abstract representations of the data.\n\nOutput layer: where the final prediction or output based on the computations performed by the hidden layers is provided.\n\nMany people think of neural networks as this mysterious black box which just predicts information. To more concretely understand what’s going on, here is a more detailed explanation what happens when you put input data through a neural network:\nInput data is put into neural networks, and neural networks are a just a collection of nested functions. These functions are defined by parameters (consisting of weights and biases).\nThink of weights as the knobs that control how much attention the neural network should pay to each feature. For example, for a neural network which classifies the fruit in images of various fruits, if the weight for the color feature is high, it means that the network considers color to be more important in the prediction. Conversely, if the weight is low, the network assigns less importance to that feature. During the training phase of a neural network, these weights are adjusted based on the input data and the desired output.\nBiases provide the neural network with the ability to make predictions even when all the input features are zero or missing. In our fruit example, biases can be thought of as an inherent inclination of the neural network to lean towards predicting one fruit over the other, regardless of the input values. They act as an additional input to each neuron in the network and help in adjusting the output of the neuron.\nThe parameters (weights and biases) are stored in tensors. Tensors are PyTorch’s version of arrays and matrices.\nThe entire process of running the input data through each of the NN’s functions to make its best guess about the correct output is known as a forward propagation, or forward pass.\nDuring training, after a forward pass, the NN goes through backward propagation: in backprop, the NN adjusts its parameters proportionate to the error in its guess. This error (known as the “loss”) is the mathematical difference between what the NN predicted and the correct answer (called the “ground truth”). The NN does this by traversing backwards from the output, back into each of the layers, collecting the derivatives of the error with respect to the parameters of the functions (parameters are like settings that control how the NN works).\nOnce the network knows how each parameter affects the error with those derivatives (called “gradients”), it can update the parameters in a way that reduces the error. This updating process is called “gradient descent.” It adjusts the parameters step by step, using the information from the gradients, to make the network better at making predictions.\nBy repeating this process with many examples and adjusting the parameters based on the gradients, the network gradually improves its performance and becomes better at making accurate predictions.\nNow, to the code!\nThe following code defines the neural network:\n\n#defining neural network\n\nclass MyNeural(nn.Module):\n    def __init__(self, input, hidden_layers, output):\n        super().__init__()\n        self.inputLayer = torch.nn.Linear(input, hidden_layers[0])\n        self.hlayers = torch.nn.ModuleDict(\n            {f\"hL{i}\": torch.nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)}\n        )\n        self.outputLayer = torch.nn.Linear(hidden_layers[-1], output)\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        x = F.relu(self.inputLayer(x))\n        for name, layer in self.hlayers.items():\n            x = F.relu(layer(x))\n        x = self.outputLayer(x)\n        y = self.sig(x)\n        return y\n\nExplanation of above code, line by line:\n\nWhen using PyTorch, which is a machine learning framework based on the Torch library, like we are here, you need to create a sub-class within the nn.Module and implement your neural network architecture within it. The name of our sub-class, our neural network architecture, is MyNeural.\nInitialize the MyNeural class. The initialization method takes in an input, which is the dimensions of the input layer, dimensions of the hidden layers, and dimensions of the output layer of the neural network. For example, in the iris dataset, the input layer will be 4 because there are 4 attributes: sepal length, sepal width, petal length, petal width (note: do not count the ‘species’ category as one of the attributes because that is what we are trying to predict with this dataset, so it should not be an input).\nThis line calls the initialization method of the parent class (nn.Module). It ensures that the necessary setup is done before defining the architecture of the custom neural network.\nThis line creates the input layer of the neural network using the torch.nn.Linear class. It specifies that the input layer has “input” number of input features and outputs “hidden_layers[0]” number of neurons.\n\nThis linear layer performs a linear transformation on the input data (an oversimplified explanation: takes the input data, factors in weights and biases of the network, and turns the data into the values of the neuron within the network). The .Linear method creates fully connected layers, where each neuron is connected to every neuron in the previous layer (see diagram at start of blog).\n\nThis line creates the hidden layers of the neural network using the torch.nn.Linear class. It creates a ModuleDict object named hlayers, which is a dictionary-like structure that holds the hidden layers. It uses a dictionary comprehension to iterate over the range of len(hidden_layers) - 1 and creates a linear layer for each hidden layer. The keys of the dictionary are formatted as hLi, where i is the index of the hidden layer, and the corresponding value is the linear layer itself. Each linear layer specifies the number of input and output features for the corresponding hidden layer.\nThis line creates the output layer of the neural network using the torch.nn.Linear class. It specifies that the output layer has hidden_layers[-1] number of input features (the output size of the last hidden layer) and output number of output features (neurons).\nSigmoid is an activation function (here we are defining the output function), which will introduce non-linearity and allow the network to capture complex relationships and make more sophisticated predictions. This line creates an instance of the nn.Sigmoid class and assigns it to the variable self.sig.\nDeclares the forward method of the class, which takes x as the input data. The forward methos is used for a “forward pass,” which is the process of inputting data into the neural network, propagating it forwards, through the hidden layers, and producing an output inference/prediction from the output layer.\nThis line applies the ReLU (Rectified Linear Unit) activation function to the input data x after passing it through the self.inputLayer linear layer. The F.relu function is a shorthand for applying the ReLU activation function element-wise. In specific, reLu is an activation function which takes any number you put into it (from the hidden layers), and if any of those numbers is negative, it turns them into positive, and if they are positive, it retains it. The reason for this is to promote efficiency and reduce the computational load of the network by focusing on relevant features and reducing redundant computations\nThis line starts a loop over the hidden layers of the neural network. It iterates through the self.hlayers dictionary, which holds the hidden layers of the network.\nInside the loop, this line applies the ReLU activation function to the intermediate results obtained by passing the data x through each hidden layer (layer). The ReLU activation function introduces non-linearity to the network. The ReLu function needs to be applied after EVERY hidden layer in order to ensure complex patterns are understood by the model, which is why we are iterating through each hidden layer using a for loop.\nAfter the loop, this line applies the output layer (self.outputLayer) to the intermediate results (x). The output layer typically performs a linear transformation on the data without applying an activation function.\nThis line applies the sigmoid activation function (self.s) to the output of the neural network (x). The sigmoid function squeezes the output values between 0 and 1, often used for binary classification or probability estimation. There are many types of activation functions, but Sigmoid is used for predicting probabilities in datasets with attributes that are categorical, like the iris dataset (e.g. sepal width, petal length, etc.)\nThis line returns the final output y from the forward method (produces the output after a forward pass).\n\n\niris = datasets.load_iris()\ndata = iris.data #assigning data (data represents features)\ntarget = iris.target #assigning target (which are class labels)\n\nHere, we the iris dataset.\nWe assign the data part of the Iris dataset to the variable data. iris.data represents the features or input variables. It is a two-dimensional array-like object where each row corresponds to a sample (flower) and each column represents a specific feature (sepal length, sepal width, petal length, petal width). After executing this line, the variable data will hold the feature data from the Iris dataset.\nNext, assign the target part of the Iris dataset. iris.target represents the class labels, which are the species of flower. Labels are like the ground truth or the correct answers that the neural network learns to predict. By providing labels during the training phase, the network learns to associate specific patterns or features in the input data with the corresponding labels. The class labels in the Iris dataset are encoded as integers, where 0 represents setosa, 1 represents versicolor, and 2 represents virginica. After executing this line, the variable target will hold the class label data from the Iris dataset.\n\ntrainD, testD, trainT, testT = train_test_split(data, target, test_size=0.1, random_state=42) \n\ntTrainD = torch.from_numpy(trainD).float() #training data\ntTrainT = torch.from_numpy(trainT).long() #training targets\ntTestD = torch.from_numpy(testD).float() #testing data\ntTestT = torch.from_numpy(testT).long() #testing targets\n\nBefore explaining the above chunk, let’s go through the process of how a neural network is trained and tested.\nAfter loading the dataset, we split the data into train, test, and validation sections (validation is just another test). So, the training will be done on one subset of the data, but the testing will be done on a different subset of the data (but within the same dataset). Doing this allows the model to use the complex relationships and patterns it identified from the training data to predict on different test data.\nIn the first line of code above, we are splitting the dataset into training and test sets (for simplification purposes, we are not splitting it into validation too), and specifying the training and testing targets (species) within the dataset as well. test_size = 0.1 ensures that 10% of the data is allocated for testing. random_state is just some arbitrary parameter that, when set again at the same value, will ensure the same exact random splits in data are used (in case you want to retest the neural network with the same splits of the dataset into test and training groups).\nThe next provided code converts the NumPy arrays representing training and testing data and targets that we made in the first line of code into PyTorch tensors. Tensors are PyTorch’s version of arrays and matrices.\nThis conversion allows for seamless integration with PyTorch and enables further processing, manipulation, and training of neural networks using the converted tensors. The .float() and .long() methods are used to ensure the appropriate data types for the tensors based on the nature of the data (float for input features (numerical data like sepal and petal length) and long for labels or targets (flower species label)).\n\n#creating custom dataset object\n\nclass myDataset():\n    def __init__(self, x, y): #initialize by putting in the dataset, which is 1 million by 4 attributes for iris \n        self.x = x\n        self.y = y\n    def __len__(self): #pytorch will look at the length (the number of rows) in dataset\n        length_ml = self.x.shape[0]\n        return(length_ml)\n    def __getitem__(self, idx): # says how to I source one training item and return it back to you, and the next time you call this function,\n        #it is going to select a different training item (keeping the training items it already used in its memory)\n        return (self.x[idx, :], self.y[idx])\n\nThe first four lines in the above code define a custom dataset class myDataset in PyTorch. The class encapsulates the input features (self.x) and labels (self.y) of the iris dataset into a single object, which helps organize and manage your data in a structured manner, making it easier to work with and reducing the risk of errors or data inconsistencies.\nThe next 3 lines of code (def len(self) …) provide methods to determine the length of the dataset. These methods are called by PyTorch to determine the length of the dataset, i.e., the number of samples in the dataset.\nThe last two lines of code (def getitem(self, idx) …) retrieve individual samples by index. The getitem method allows you to define how individual samples are accessed from the dataset. This customization is valuable when you need to implement specific data preprocessing or transformations before feeding the data into the neural network.\n\ntrainDataset = myDataset(tTrainD, tTrainT)\ntrainDataset\n\n&lt;__main__.myDataset at 0x12edbcd10&gt;\n\n\nBy passing tTrainD (training data) and tTrainT (training targets) as arguments, you are initializing the trainDataset object with the corresponding data and targets in preparation for training.\nExplanation of the process of training neural networks:\nWhen training a neural network, the process of splitting the data into batches is an essential step that enables efficient training. Instead of processing the entire dataset at once, the data is divided into smaller subsets called batches. Each batch contains a fixed number of samples, and the network is updated based on the gradients computed from the predictions and the corresponding targets within the batch.\nThe process of training a neural network with batching typically involves the following steps:\nData Loading: The training dataset is loaded, either as a whole or through a data loader object, which provides access to the data in batches. The data loader takes care of shuffling, batching, and any necessary preprocessing.\nMini-batch Iteration: The training data is divided into mini-batches, each containing a predefined number of samples (specified by the batch size). The network will process one mini-batch at a time.\nForward Pass: For each mini-batch, the input data is fed forward through the network. The network computes predictions for the samples in the mini-batch.\nLoss Computation: The predictions from the forward pass are compared to the corresponding target values (labels) for the samples in the mini-batch. This comparison generates a loss value, which quantifies the error between the predicted outputs and the true targets.\nBackward Pass and Parameter Update: The loss is used to compute gradients with respect to the network parameters. The gradients indicate the direction and magnitude of the updates required to minimize the loss. The gradients are backpropagated through the network using the chain rule of derivatives. The network’s parameters are then updated using an optimization algorithm (e.g., gradient descent or its variants) based on these gradients.\nIteration: Steps 3 to 5 are repeated for each mini-batch in the training data. This process is typically performed for multiple epochs, where an epoch refers to one complete pass through the entire training dataset. The network updates its parameters after each mini-batch, gradually improving its performance over the epochs.\n\ntrain_loader = DataLoader(trainDataset, batch_size=8, shuffle=True)\ntrain_loader\n\n&lt;torch.utils.data.dataloader.DataLoader at 0x14c9dbe90&gt;\n\n\ntrainDataset: This is the dataset object that you want to load using the data loader. It should be an instance of a PyTorch dataset class. We already defined trainDataset st chunk of code using the myDataset custom object we created.\nbatch_size: Specifies the number of samples to load in each batch. In this case, each batch will contain 8 samples.\nshuffle: If set to True, the data loader will shuffle the samples before each epoch (a complete iteration over the dataset). Shuffling the data helps in randomizing the order of the samples and can improve the model’s training performance.\nEssentially, the training examples are split into groups called “batches”. This allows the dataset to be “loaded” into the network, or processed, in more manageable, smaller chunks. Those batches then produce some output values, get backpropagated, and each and every batch individually goes through the NN in each epoch. An epoch is one pass through your entire training data. So if there are 30 epochs, you are training 30 times.\n\n#naming the neural network and setting dimensions of layers\nmyNN = MyNeural(4, [5, 6, 7, 6], 4)\nmyNN\n\nMyNeural(\n  (inputLayer): Linear(in_features=4, out_features=5, bias=True)\n  (hlayers): ModuleDict(\n    (hL0): Linear(in_features=5, out_features=6, bias=True)\n    (hL1): Linear(in_features=6, out_features=7, bias=True)\n    (hL2): Linear(in_features=7, out_features=6, bias=True)\n  )\n  (outputLayer): Linear(in_features=6, out_features=4, bias=True)\n  (sig): Sigmoid()\n)\n\n\nIn the above code, for the neural network model ‘MyNeural’ which we defined earlier:\nWe set 4 input features (4 because of the number attributes of the iris dataset: sepal width, sepal length, petal width, and petal length). The 4 input features produce 5 features (aka 5 neurons) in the first hidden layer, 6 neurons in the second hidden layer, 7 in the third, 6 in the fourth, and finally output 4 features as a prediction. So, the values in the list [5, 6, 7, 6] assign the number of neurons in each hidden layer, and the length of the list corresponds to the number of hidden layers. Here, there are 4 hidden layers. These dimensions for MyNeural are all assigned to myNN – which becomes the name of the specific neural network.\n\nopt = torch.optim.SGD(myNN.parameters(), lr = 0.01)\nlossfunction = nn.CrossEntropyLoss()\n\nBased on the gradients computed by backpropagation, an optimizer is an algorithm or method used to adjust the parameters of the model during the training process in order to minimize the loss (to produce a more accurate prediction). In the above code, we are initializing an optimizer object (opt) of type Stochastic Gradient Descent (SGD). It takes two arguments:\nmyNN.parameters(): This specifies the parameters of your neural network model (myNN) that will be optimized during training. The optimizer will update these parameters based on the computed gradients. lr=0.01: This sets the learning rate for the optimizer. The learning rate determines the step size taken during optimization, influencing how quickly the model learns and converges.\nEven though we used the SGD optimizer here, there other examples of optimizers including Adam, RMSprop, Adagrad, etc., all of which have their own strengths and weaknesses.\nThe loss function calculates the discrepancy between the predicted outputs and the ground truth labels, providing a measure of the model’s performance during training. The CrossEntropyLoss() function from torch.nn is commonly used for multi-class classification tasks, like the classification of species that we are doing with the iris dataset.\n\n#training loop for an NN model\n\nnum_epochs = 1000\ntrain_loss = []\ntest_loss = []\n\nfor epoch in range(num_epochs):\n    myNN.train()\n    running_loss = 0\n    dt_size = 0\n    for i, (batchX, batchY) in enumerate(train_loader): \n        #batchY represents target (actual) labels corresponding to input data batch (batchX)\n        opt.zero_grad()\n        output = myNN(batchX)\n        loss = lossfunction(output, batchY)\n        loss.backward()\n        opt.step()\n        running_loss += loss.item() * batchX.size(0)\n        dt_size += batchX.size(0)\n    train_loss.append(running_loss / dt_size)\n\n    myNN.eval()\n    with torch.no_grad():\n        p = myNN(tTestD) #forward pass on testing data subset\n        l = lossfunction(p, tTestT) #loss calculation\n        test_loss.append(l.item())\n\nThe above code essentially iterates over the training data, performs forward and backward passes, updates the model parameters, and calculates and stores the training and testing loss for each epoch.\nAn explanation of the code, line by line:\n\nnum_epochs = 1000: This variable indicates the number of training epochs, specifying how many times the entire dataset will be iterated (repeatedly ran through the NN) during training.\ntrain_loss and test_loss are empty lists that will store the training and testing loss values for each epoch, respectively, within the for loop later.\nThe loop for epoch in range(num_epochs): iterates over the specified number of epochs.\nmyNN.train() sets the neural network model (myNN) in training mode.\nrunning_loss and dt_size variables are initialized to track the cumulative loss and the total size of the training dataset.\nThe inner loop for i, (batchX, batchY) in enumerate(train_loader): iterates over the batches of data from the training data loader (train_loader). The enumerate(train_loader) part adds an index counter (i) to each batch returned by the train_loader. This means that as you iterate over the batches in the train_loader, you also have access to the index or position of the current batch. The index counter (i) starts from 0 and increments by 1 for each batch in the train_loader. It allows you to keep track of the progress and index of the current batch within the training loop.\nopt.zero_grad() clears the gradients of the optimizer before calculating the new gradients after every batch. This is VERY important to include because during backpropagation, gradients are calculated and stored for each parameter of the model. If the gradients are not cleared, they would accumulate from one iteration to the next. This would result in incorrect gradient values and lead to incorrect updates of the model parameters. Many people forget it!\noutput = myNN(batchX) computes the forward pass of the neural network model on the current batch of inputs (batchX).\nloss = lossfunction(output, batchY) calculates the loss between the predicted outputs and the actual labels (batchY) using the specified loss function (lossfunction).\nloss.backward() performs backpropagation, computing the gradients of the model’s parameters with respect to the loss.\nopt.step() updates the model’s parameters by taking an optimization step using the optimizer (opt).\nrunning_loss += loss.item() * batchX.size(0) and dt_size += batchX.size(0) accumulate the loss and the size of the current batch for later calculation of the average loss.\ntrain_loss.append(running_loss / dt_size) calculates and stores the average training loss for the current epoch.\nmyNN.eval() switches the model to evaluation mode. During the training phase of a neural network, the model undergoes iterations to learn from the training data and update its parameters. However, when it comes to evaluating the model’s performance on a validation or test set, it is important to ensure that the model behaves differently compared to the training phase. This is where the evaluation mode comes into play.\nwith torch.no_grad(): ensures that no gradients are computed during the following evaluation phase.\np = myNN(tTestD) performs the forward pass of the model on the testing dataset (tTestD) to obtain the predicted outputs.\nl = lossfunction(p, tTestT) calculates the loss between the predicted outputs and the testing labels (tTestT).\ntest_loss.append(l.item()) stores the testing loss for the current epoch.\n\n\n#plotting loss with training and testing the NN\n\nplt.plot(train_loss, label='train loss')\nplt.plot(test_loss, label = 'test loss')\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Testing and Training Loss for NN\")\n\nText(0.5, 1.0, 'Testing and Training Loss for NN')\n\n\n\n\n\n\n#testing NN\n\nprint(\"Testing:\")\nopt.zero_grad()\noutput = myNN(tTestD)\nprint(tTestD.shape)\nloss = lossfunction(output, tTestT)\n\nTesting:\ntorch.Size([15, 4])\n\n\nOverall, the code snippet performs a forward pass of the neural network model on the testing data, prints the shape of the testing data, and calculates the loss between the predicted outputs and the target labels. This can be useful for evaluating the performance of the model on the testing data after training.\nPrinting the shape of the testing data is crucial as it allows for data verification, debugging, and input size considerations. It helps ensure that the testing data matches the expected input shape for the neural network model. Additionally, it provides insights into the number of samples and dimensions, which is vital for designing and configuring the model. Checking the data shape ensures that preprocessing steps have been correctly applied.\n\n#analyzing and printing results for one epoch\n\npredictions = []\nfor row in output:\n    if row.max() == row[0]:\n        predictions.append(0)\n    elif row.max() == row[1]:\n        predictions.append(1)\n    else:\n        predictions.append(2)\ntPreds = torch.tensor(predictions).view(15,1)\ntTargets = tTestT.view(15,1)\n\nresult = torch.cat([tPreds,tTargets], dim=1)\nprint(result)\ncorrect = 0\nfor row in result:\n    if row[0] == row[1]:\n        correct += 1\n\nprint(correct)\n\ntensor([[1, 1],\n        [0, 0],\n        [1, 2],\n        [1, 1],\n        [1, 1],\n        [0, 0],\n        [1, 1],\n        [1, 2],\n        [1, 1],\n        [1, 1],\n        [1, 2],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [0, 0]])\n12\n\n\nThe above code snippet is not necessary but can be helpful to understand the accuracy of the model’s prediction. The code snippet calculates the predicted labels based on the output tensor, compares them with the target labels, and prints the resulting tensor as well as the number of correct predictions."
  },
  {
    "objectID": "posts/command-line-skills/index.html",
    "href": "posts/command-line-skills/index.html",
    "title": "Practicing Working with Terminal",
    "section": "",
    "text": "Working with Terminal ( + GitHub & VSCode)\nThe goal of this 8 week program is to be able to train an LLM to take in a full DNA sequence and predict some biological mechanism, such as gene expression or the effect of transcription factor binding. This can only be accomplished by using a supercomputer with many, many GPUs. These supercomputers will take in the training code from the terminal (or VSCode), which is why it is necessary to learn how to use VSCode and navigate the command line on the terminal of the laptop.\nFirst, you need to ensure that your new file that you are working on is in the correct environment. An environment is kind of like a storage area on your laptop for all your programming tools (e.g. Python)– it’s basically like a folder for everything you need.\nConda: Conda is an environment management tool which ensures that all your code, program, and files for a specific project are in the same environment. It also manages the installation, updating, and removal of packages (e.g. numpy, pandas, etc.). Conda comes with Anaconda and Miniconda, which you can download from the internet and have all the tools you would need to explore, model, and visualize datasets and more.\n\nMake a New Environment\nOpen terminal. Type the following, where “envname” is what you want to call your new environment:\nconda create —name envname\n\n\nActivate Newly Created Environment\nDo this to enter your desired environment. Type the following, where “envname” is your environment name:\nconda activate envname\n\n\nInstalling Tools for Programming in Python within New Environment\nType the following:\nconda install python\n\n\nInstall “pip” before installing packages for Python\npip is is a package management system used to install and manage software packages written in Python. It is a command-line tool that comes bundled with Python installations.\nTo install pip:\nconda install pip\nNote: Make sure you are in the desired environment while doing this.\nYou can check that you are in that desired environment if the environment name is in the parenthesis before the remaining line of code produced by terminal. So, after you activate the environment, it should produce something like this:\n(envname) Your-MacBook-Pro:~ macusername$\nWhen you are in the “normal” default environment, it will look something like this:\n(base) Your-MacBook-Pro:~ macusername$\n\n\nVarious pip commands\nHere are some commonly used pip commands:\n\npip install package_name: Installs a package from PyPI or another source.\n\n**For example, for installing numpy: python -m pip install numpy\nNote: The -m flag is commonly used for running Python scripts that are part of a package or when you want to ensure that the module is executed with the correct environment and dependencies.\n\npip uninstall package_name: Uninstalls a package.\npip list: Lists installed packages.\npip search package_name: Searches PyPI for packages matching the given name.\npip show package_name: Displays information about a specific package.\npip freeze: Generates a requirements.txt file containing a list of installed packages and their versions.\n\n\n\nUsing VSCode with Desired Environment\nVSCode has an integrated terminal that allows you to run commands directly within the editor. However, VSCode’s integrated terminal supports various shells, including PowerShell, Command Prompt (Windows), and Bash (macOS/Linux). So, while coding in VSCode, you need to make sure your code file is in the correct environment for your project. To do this, click on the bottom right of the screen on VSCode (in this example, the tab that says “3.11.3 (‘dlgtools’:conda). dlgtools is the name of my desired project environment. If the name of the environment is not the environment you want to work in, simply click on it and VSCode should open a tab at the top of your screen which says”Select Interpreter” and you can switch into a different environment. Your file will then be stored in this environment, so any packages or programs (like Python) which you plan to use in that file should be in that environment.\n\n\n\nChecking to See If You Have Python\nOnce the terminal is open, type python --version or python3 --version and press Enter.\n\nIf Python is installed, the command will display the version number of Python installed on your system. For example, you might see something like Python 3.9.2.\nIf Python is not installed or if the command is not recognized, you will typically see an error message indicating that the command is not found or recognized. In this case, you’ll need to install Python.\n\n\n\nChecking Your VSCode in Terminal\nIt is usually very difficult to see the output of your code in the integrated terminal of VSCode because it is kind of obscured among the lines about your system, device username, etc. To more clearly see the outputs of your code, you can do the following options:\n\nCheck the code outputs directly in your device’s terminal.\n\n\nOpen terminal.\nMaking sure you are in the desired environment (check the parenthesis), type the following, where file_name is the name of the VSCode file you want to check the code for:\nfrom file_name import *\nThis should clearly produce all the outputs for your code\n\n\nCopy-paste the code into Jupyter Notebook, Google Colab, or some other software with all the packages and tools built in and run the code.\nAdd the following line of code at the very top and very bottom of your coding file to create some space between the outputs within the integrated terminal of VSCode itself. This may not be as helpful to clearly see the code, but it may make a slight difference in visibility.\n\n\nprint(“\\n \\n \\n --------------- \\n \\n \\n”)\n\n\n\n\nFile Navigation in Terminal\nHere are some basic commands in your device’s terminal to make sure you are storing all your files within the desired directory. A directory is basically a type of folder on your device. You need to know which directory you are putting your project files so that you do not lose any important files in the short or long-term.\n\n\nAdditional Navigation Commands in Terminal:\nopen . → opens the directory you are currently in\ncd .. → goes to the parent directory (“steps back”)\nls -a → see all files within the directory (including hidden ones with .git)\ncd \\ → goes to the root directory of the hardware system (the furthest back root)\nBasically, I would start by typing “pwd” in Terminal to determine which directory I am currently in. If you need to move back into an earlier parent folder/directory, type cd ..\nIf you want to move into a further directory, type cd directory_name.\nIf you do not know the name of the directory you want to move further into, or if you do not know if that directory is within your current working directory, type ls to see all directories/files within your current working directory.\nls-a shows all the hidden files as well. Hidden files are files which work in the background of your project, and will begin with a . , which as .git or .nojekyll.\nAfter navigating into the root directory using cd \\, and then going into the desired directory, you can type “echo $PATH” to get the path to get to that directory.\n\n\n\nGithub/Git Commands in Terminal\n\nCloning on GitHub\nCloning refers to creating a local copy of an entire repository, including all its files, commit history, branches, and configuration. When you clone a repository, you create an identical copy on your local machine. This allows you to work with the project, make changes, commit them, and push them back to the remote repository. Cloning is typically used when you want to contribute to a project or work on your own project locally.\nTo clone a repository from GitHub, follow these steps:\n\nOpen the GitHub repository page in your web browser.\nClick on the “Code” button, located near the top-right corner of the repository page.\nClick on the clipboard icon to copy the repository’s URL. Alternatively, you can click on the “Download ZIP” button to download a compressed version of the repository instead of cloning it with Git.\nOpen your terminal or Git Bash (if you’re on Windows).\nNavigate to the directory where you want to clone the repository. You can use the cd command to change directories.\nOnce you’re in the desired directory, use the following command to clone the repository:\n\ngit clone &lt;paste_repository_URL&gt;\n\n\nAfter making changes to local files on your device, you want to sync those changes to the remote, master directory on Github. Git is a tools which allows this syncing. Carry out the following steps to do so:\n\ngit add * → add your changes\ngit status → check you are adding the files/directories you want\ngit commit -m ‘message’ → add a message\ngit push → update the master directory with your work\n\n\n\nAnother option – “pulling” in Github:\nPulling is the opposite of pushing: it’s what you do when the remote, master directory (maybe owned by someone other than you) has changes and you want to update your local directory with those changes. Typically used in collaborative files.\n\ngit pull → update your local directory with the master (remote) directory\n\n\n\nWhen would you pull?\nThe `git pull` command is used to update your local repository with the latest changes from a remote repository, typically the one you cloned from. It incorporates changes made by others and brings your local copy up to date.\nYou would use `git pull` in a few different scenarios:\n1. **Working on a shared project**: If you are collaborating with other people on a project, they might have made changes to the remote repository that you want to sync with. Running `git pull` will fetch those changes and merge them into your local branch.\n2. **Staying up to date**: Even if you’re not collaborating with others, it’s good practice to regularly update your local repository with the latest changes from the remote repository. This ensures that you have the most recent version of the code and can avoid conflicts when you eventually push your own changes.\n3. **Resolving conflicts**: Sometimes, when you pull changes from the remote repository, there might be conflicts between your local changes and the incoming changes. For example, if someone edited the same lines which you edited, and now there are different versions of the same file in the repo. In such cases, Git will notify you of the conflicts and provide an opportunity to resolve them manually.\nIt’s important to note that before running `git pull`, you should commit your local changes to avoid conflicts. If you have uncommitted changes, Git may ask you to stash or commit them before pulling.\n\n\nForking in Github:\nForking a file in GitHub is like making a personal copy of someone else’s file or project. When you fork a file, you create your own version of it that you can modify and make changes to without affecting the original file.\nHere’s a simple analogy: Imagine you have a friend who has a really cool drawing. You want to add your own touches and modifications to that drawing, but you don’t want to mess up your friend’s original. So, what you do is make a photocopy of the drawing and work on that copy. This way, you can freely experiment and make changes without worrying about ruining the original.\nIn GitHub, forking is similar. If you find a file or project in someone else’s repository that you want to modify or contribute to, you can fork it to create your own personal copy of that repository. This copy will be stored in your GitHub account, and you can make changes without affecting the original file or the owner’s repository.\nOnce you’ve made the desired changes to your forked repository, you can choose to share those changes with the original owner through a process called a pull request. This allows the owner to review your changes and decide whether to incorporate them into the original file or project.\n\n\nHow to fork a file on Github?\nTo fork a repository in GitHub, including all its files, branches, and commit history, follow these steps:\n\nOpen your web browser and go to the GitHub repository page that contains the file you want to fork.\nIn the top-right corner of the repository page, click on the “Fork” button.\nGitHub will prompt you to select where you want to fork the repository. Choose your user account or any organization you belong to. Click on the appropriate option.\nGitHub will then create a copy of the repository under your account or organization. Once the forking process is complete, you will be redirected to the forked repository’s page.\n\nNow you have successfully forked the entire repository, not just an individual file. You will have a separate copy of the repository in your GitHub account. This copy will include all the files, branches, and commit history present in the original repository.\nYou can make changes to the files, add new features, fix bugs, or experiment with the forked repository as you see fit. You can commit and push changes to the forked repository without affecting the original repository"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Language Models in Molecular Biology",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\nTrain and Logistic Predictor\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHow to Extract a DNA Sequence Centered at a Specific Locus\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nNeural Networks Code and Explanation\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nSrusti\n\n\n\n\n\n\n  \n\n\n\n\nGWAS Code\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nPracticing Working with Terminal\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nThe Biology to be Explored in LLMs\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nBasics of LLMs and Their Role in the Field of Biology\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My ‘LLM in Biology’ Blog!\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\nSrusti Donapati\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is essentially a project of progress reports on biological knowledge and computational skills that I have acquired over my 8 weeks in the Im Lab during Summer 2023 for the GPT in Genomics Project."
  },
  {
    "objectID": "posts/test_post/index.html",
    "href": "posts/test_post/index.html",
    "title": "test post",
    "section": "",
    "text": "#code"
  }
]